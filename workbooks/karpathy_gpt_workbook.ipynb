{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pMifN8YNyDBK",
        "xCX_5YoUzA8l",
        "yLRBrvo_xxhC",
        "wa7R6uB6yCrp",
        "Gioq9w7zyDB4",
        "rA2lOBuCyDYU",
        "Eo7bhYUP3uqW",
        "Sj9IRBIJ3vFE",
        "zMSSSYTF3vT6",
        "R_u1Dd6-52uV",
        "B_2jubCN52uW",
        "GsiE7SZx52uY",
        "asvSFWZ952uY",
        "6dTG8_6O52uY",
        "PYDs9zxr52uZ",
        "E-e9Tblq52uZ",
        "OB9qent_52uZ",
        "5PsvXR8b52ua"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Workbook Overview\n",
        "\n",
        "This workbook guides you through coding a Transformer-based GPT model from scratch. It is based on Andrej Karpathy's \"Zero to Hero\" YouTube tutorial titled [\"Let's build GPT: from scratch, in code, spelled out\"](https://www.youtube.com/watch?v=kCc8FmEb1nY).\n",
        "\n",
        "### Who Is This For?\n",
        "\n",
        "- Those familiar with Andrej Karpathy’s \"Zero to Hero\" series and looking to code GPT from scratch.\n",
        "\n",
        "### How to Use This Workbook\n",
        "\n",
        "- The workbook is organized into three main sections: **Coding Instructions**, **Coding Exercises**, and **Code Solutions**.\n",
        "- **Coding Instructions**: Step-by-step guidance on building the key components of a GPT model.\n",
        "- **Coding Exercises**: Implement the code as you progress through the instructions.\n",
        "- **Code Solutions**: Refer to the full code solutions for validation or help if you get stuck.\n",
        "- Use the Colab Table of Contents for seamless navigation.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- Gain a thorough understanding of how Transformer-based language models like GPT are constructed.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_3VhVGiPjWA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List of instructions"
      ],
      "metadata": {
        "id": "pMifN8YNyDBK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNF72tZHFHhk"
      },
      "outputs": [],
      "source": [
        "# Let's Build GPT (Step-by-Step Transformer Language Model)\n",
        "\n",
        "## Part 1: Setup and Data Preparation\n",
        "\n",
        "# 1. Imports & Configurations\n",
        "#    - Import necessary libraries (`torch`, `torch.nn`, `torch.nn.functional`, etc.).\n",
        "#    - Set device configuration (CPU/GPU).\n",
        "#    - Set random seed for reproducibility.\n",
        "\n",
        "# 2. Download Dataset\n",
        "#    - Download and load the \"tinyshakespeare\" dataset.\n",
        "#    - https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "#    - Read the text data from the file.\n",
        "\n",
        "# 3. Vocabulary Creation\n",
        "#    - Extract unique characters from the dataset.\n",
        "#    - Determine the vocabulary size.\n",
        "\n",
        "# 4. Tokenizer\n",
        "#    - Create mappings for character-to-index (`stoi`) and index-to-character (`itos`).\n",
        "#    - Implement `encode()` and `decode()` functions for tokenization.\n",
        "\n",
        "# 5. Train and Test Splits\n",
        "#    - Convert the entire dataset into token indices.\n",
        "#    - Split the data into training and validation sets (90/10 split).\n",
        "\n",
        "# 6. Dataloader\n",
        "#    - Define a function `get_batch()` to generate batches of data for training and evaluation.\n",
        "#    - Ensure that each batch contains sequences of fixed block size.\n",
        "\n",
        "## Part 2: Building the Initial GPT Model\n",
        "\n",
        "# 7. Model: Embedding Layer and Output Linear Transformation\n",
        "#    - Implement the GPT class with an embedding layer and a linear layer for output.\n",
        "#    - Forward pass should handle both token embeddings and linear transformation to logits.\n",
        "\n",
        "# 8. Generate Function\n",
        "#    - Implement the `generate()` function to generate text using the trained model.\n",
        "#    - The function should iterate over a specified number of new tokens, updating the input sequence each time.\n",
        "\n",
        "## Part 3: Training and Evaluation\n",
        "\n",
        "# 9. Evaluation Loop\n",
        "#    - Implement the `estimate_loss()` function to evaluate the model on training and validation data.\n",
        "#    - Ensure that the model is in evaluation mode during this process.\n",
        "\n",
        "# 10. Training Loop\n",
        "#    - Set up the training loop with AdamW optimizer.\n",
        "#    - Include periodic evaluation using `estimate_loss()` and print training/validation losses.\n",
        "#    - Generate sample from the model\n",
        "\n",
        "\n",
        "## Part 4: Enhancing the GPT Model\n",
        "\n",
        "# 11. Model: Positional Embeddings\n",
        "#    - Add positional embeddings to the model to capture the order of tokens.\n",
        "#    - Update the forward pass to combine token embeddings with positional embeddings.\n",
        "\n",
        "# 12. Model: Single Attention Head\n",
        "#    - Implement a single attention head within the model to capture token relationships.\n",
        "#    - Incorporate this attention mechanism into the forward pass.\n",
        "\n",
        "# 13. Model: Multi-Head Attention\n",
        "#    - Expand the model to include multiple attention heads.\n",
        "#    - Implement a projection layer to combine the outputs of the multiple heads.\n",
        "\n",
        "## Part 5: Building the Transformer Block\n",
        "\n",
        "# 14. Model: MLP\n",
        "#    - Add a multi-layer perceptron (MLP) to the model.\n",
        "#    - The MLP should consist of a projection up, ReLU activation, and a projection down.\n",
        "\n",
        "# 15. Model: Transformer Block\n",
        "#    - Combine multi-head attention and MLP into a single Transformer block.\n",
        "#    - Use this block in the model to stack multiple layers.\n",
        "\n",
        "## Part 6: Final Enhancements\n",
        "\n",
        "# 16. Model: Skip Connections, normalization and dropout\n",
        "#    - Implement skip connections (residual connections) around the attention and MLP layers.\n",
        "#    - Add layer normalization before applying the skip connections to stabilize training.\n",
        "#    - Include dropout layers in both the attention and MLP layers to prevent overfitting.\n",
        "\n",
        "## Part 7: Final Training and Evaluation\n",
        "\n",
        "# 17. Final Evaluation and Text Generation\n",
        "#    - Train the final GPT model with the full architecture.\n",
        "#    - Evaluate the final model on the validation set.\n",
        "#    - Use the trained model to generate new text samples."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding Exercises"
      ],
      "metadata": {
        "id": "xCX_5YoUzA8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setup and Data Preparation"
      ],
      "metadata": {
        "id": "yLRBrvo_xxhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Imports & Configurations"
      ],
      "metadata": {
        "id": "QRb3Ey5DyBqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports & Configurations\n",
        "#    - Import necessary libraries (`torch`, `torch.nn`, `torch.nn.functional`, etc.).\n",
        "#    - Set device configuration (CPU/GPU).\n",
        "#    - Set random seed for reproducibility.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "WbQmpUcdyBqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Download Dataset"
      ],
      "metadata": {
        "id": "6NG_a-2nyB5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Download Dataset\n",
        "#    - Download and load the \"tinyshakespeare\" dataset.\n",
        "#    - https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "#    - Read the text data from the file.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "EfnkwddQyB5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Vocabulary Creation"
      ],
      "metadata": {
        "id": "sXjVs47FyCOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Vocabulary Creation\n",
        "#    - Extract unique characters from the dataset.\n",
        "#    - Determine the vocabulary size.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "vouCCDNCyCOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Tokenizer"
      ],
      "metadata": {
        "id": "vQt3_YQlyCXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Tokenizer\n",
        "#    - Create mappings for character-to-index (`stoi`) and index-to-character (`itos`).\n",
        "#    - Implement `encode()` and `decode()` functions for tokenization.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "OGJ9ubqpyCXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Train and Test Splits"
      ],
      "metadata": {
        "id": "IcRVeRFLyCd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Train and Test Splits\n",
        "#    - Convert the entire dataset into token indices.\n",
        "#    - Split the data into training and validation sets (90/10 split).\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "GNBIdh4FyCd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Dataloader"
      ],
      "metadata": {
        "id": "Sx-NlSSoyCke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Dataloader\n",
        "#    - Define a function `get_batch()` to generate batches of data for training and evaluation.\n",
        "#    - Ensure that each batch contains sequences of fixed block size.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "viIh2BlsyCke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Building the Initial GPT Model"
      ],
      "metadata": {
        "id": "wa7R6uB6yCrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Model: Embedding Layer and Output Linear Transformation"
      ],
      "metadata": {
        "id": "a_o3NxYtyCyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Model: Embedding Layer and Output Linear Transformation\n",
        "#    - Implement the GPT class with an embedding layer and a linear layer for output.\n",
        "#    - Forward pass should handle both token embeddings and linear transformation to logits.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "mQStio1WyCyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Generate Function"
      ],
      "metadata": {
        "id": "L6N8Ev2UyC5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Generate Function\n",
        "#    - Implement the `generate()` function to generate text using the trained model.\n",
        "#    - The function should iterate over a specified number of new tokens, updating the input sequence each time.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "ANRzCUtoyC5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Training and Evaluation"
      ],
      "metadata": {
        "id": "Gioq9w7zyDB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Evaluation Loop"
      ],
      "metadata": {
        "id": "0A9kJAdxyDI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Evaluation Loop\n",
        "#    - Implement the `estimate_loss()` function to evaluate the model on training and validation data.\n",
        "#    - Ensure that the model is in evaluation mode during this process.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "vvD3-3BsyDI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Training Loop"
      ],
      "metadata": {
        "id": "8kDqLmhJyDQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Training Loop\n",
        "#    - Set up the training loop with AdamW optimizer.\n",
        "#    - Include periodic evaluation using `estimate_loss()` and print training/validation losses.\n",
        "#    - Generate sample from the model\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "R78gAfNXyDQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Enhancing the GPT Model"
      ],
      "metadata": {
        "id": "rA2lOBuCyDYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Model: Positional Embeddings"
      ],
      "metadata": {
        "id": "gMkSSJLbyDfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Model: Positional Embeddings\n",
        "#    - Add positional embeddings to the model to capture the order of tokens.\n",
        "#    - Update the forward pass to combine token embeddings with positional embeddings.\n",
        "\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "Md-mTrRmyDfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Model: Single Attention Head"
      ],
      "metadata": {
        "id": "AHetfG-xyDpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Model: Single Attention Head\n",
        "#    - Implement a single attention head within the model to capture token relationships.\n",
        "#    - Incorporate this attention mechanism into the forward pass.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "RJW_SPJlyDpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Model: Multi-Head Attention"
      ],
      "metadata": {
        "id": "ntgdmFCiyD4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Model: Multi-Head Attention\n",
        "#    - Expand the model to include multiple attention heads.\n",
        "#    - Implement a projection layer to combine the outputs of the multiple heads.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "T3ChSrJzyD4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Building the Transformer Block"
      ],
      "metadata": {
        "id": "Eo7bhYUP3uqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. Model: MLP"
      ],
      "metadata": {
        "id": "SUeZo3ti3uqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Model: MLP\n",
        "#    - Add a multi-layer perceptron (MLP) to the model.\n",
        "#    - The MLP should consist of a projection up, ReLU activation, and a projection down.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "f1utFvjC3uqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. Model: Transformer Block"
      ],
      "metadata": {
        "id": "fSuNZH5i3uqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Model: Transformer Block\n",
        "#    - Combine multi-head attention and MLP into a single Transformer block.\n",
        "#    - Use this block in the model to stack multiple layers.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "HS9Vw3Ct3uqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Final Enhancements"
      ],
      "metadata": {
        "id": "Sj9IRBIJ3vFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16: Model: Skip Connections, normalization and dropout"
      ],
      "metadata": {
        "id": "Cx5PX10Z3vFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Model: Skip Connections, normalization and dropout\n",
        "#    - Implement skip connections (residual connections) around the attention and MLP layers.\n",
        "#    - Add layer normalization before applying the skip connections to stabilize training.\n",
        "#    - Include dropout layers in both the attention and MLP layers to prevent overfitting.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "dI6lz3hC3vFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7: Final Training and Evaluation"
      ],
      "metadata": {
        "id": "zMSSSYTF3vT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. Final Evaluation and Text Generation"
      ],
      "metadata": {
        "id": "zhvqzYVB3vT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. Final Evaluation and Text Generation\n",
        "#    - Train the final GPT model with the full architecture.\n",
        "#    - Evaluate the final model on the validation set.\n",
        "#    - Use the trained model to generate new text samples.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "8Tl8Gnao3vT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Solutions"
      ],
      "metadata": {
        "id": "R_u1Dd6-52uV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setup and Data Preparation"
      ],
      "metadata": {
        "id": "B_2jubCN52uW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Imports & Configurations"
      ],
      "metadata": {
        "id": "X46qwYnT52uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports & Configurations\n",
        "#    - Import necessary libraries (`torch`, `torch.nn`, `torch.nn.functional`, etc.).\n",
        "#    - Set device configuration (CPU/GPU).\n",
        "#    - Set random seed for reproducibility."
      ],
      "metadata": {
        "id": "25ybxyyp52uX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Oq7g6B4m6erP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Download Dataset"
      ],
      "metadata": {
        "id": "vUdzeam152uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Download Dataset\n",
        "#    - Download and load the \"tinyshakespeare\" dataset.\n",
        "#    - https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "#    - Read the text data from the file."
      ],
      "metadata": {
        "id": "K2d_biFF52uX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnYV6WGG6hZr",
        "outputId": "8f0070b2-0ee2-43f8-930f-4a9d8b753353"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-16 17:33:57--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-08-16 17:33:58 (50.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Vocabulary Creation"
      ],
      "metadata": {
        "id": "MJUr1ZFe52uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Vocabulary Creation\n",
        "#    - Extract unique characters from the dataset.\n",
        "#    - Determine the vocabulary size."
      ],
      "metadata": {
        "id": "Esj5DybP52uX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)\n",
        "print(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_hxIY5K6n14",
        "outputId": "e19a8bec-d7b2-47e8-bf53-1be442d0a479"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Tokenizer"
      ],
      "metadata": {
        "id": "bW4dNqXj52uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Tokenizer\n",
        "#    - Create mappings for character-to-index (`stoi`) and index-to-character (`itos`).\n",
        "#    - Implement `encode()` and `decode()` functions for tokenization."
      ],
      "metadata": {
        "id": "26eXWiTz52uX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "print(encode('hello world!'))\n",
        "print(decode(encode('hello world!')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0iTekdh6zwR",
        "outputId": "b9d11eb1-2b9a-4685-8842-c0dca054ff4c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
            "hello world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Train and Test Splits"
      ],
      "metadata": {
        "id": "qdYOjzud52uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Train and Test Splits\n",
        "#    - Convert the entire dataset into token indices.\n",
        "#    - Split the data into training and validation sets (90/10 split)."
      ],
      "metadata": {
        "id": "zNHmkJd252uX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(len(data)*0.9)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWPSFAXlNBsI",
        "outputId": "d0c6adbb-7efc-44f2-d6a2-1430c479507f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1003854])\n",
            "torch.Size([111540])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Dataloader"
      ],
      "metadata": {
        "id": "QuGzW_GD52uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Dataloader\n",
        "#    - Define a function `get_batch()` to generate batches of data for training and evaluation.\n",
        "#    - Ensure that each batch contains sequences of fixed block size."
      ],
      "metadata": {
        "id": "15uLpTIE52uY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "\n",
        "    data = train_data if split=='train' else val_data\n",
        "\n",
        "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
        "\n",
        "    x = torch.stack([data[i: i+ block_size] for i in ix], dim=0)\n",
        "    y = torch.stack([data[i+1: i+ block_size+1] for i in ix], dim=0)\n",
        "\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "print(get_batch('train'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9U87Nq569rs",
        "outputId": "d91052d7-f5fa-437e-8f5e-da508df7a3b7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]]), tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Building the Initial GPT Model"
      ],
      "metadata": {
        "id": "GsiE7SZx52uY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Model: Embedding Layer and Output Linear Transformation"
      ],
      "metadata": {
        "id": "Djtzojbq52uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Model: Embedding Layer and Output Linear Transformation\n",
        "#    - Implement the GPT class with an embedding layer and a linear layer for output.\n",
        "#    - Forward pass should handle both token embeddings and linear transformation to logits.\n",
        "# See next cell"
      ],
      "metadata": {
        "id": "I-YdYLW-52uY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Generate Function"
      ],
      "metadata": {
        "id": "2I0mmsfs52uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Generate Function\n",
        "#    - Implement the `generate()` function to generate text using the trained model.\n",
        "#    - The function should iterate over a specified number of new tokens, updating the input sequence each time.\n",
        "\n",
        "n_embed = 32\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None): # x shape (B, T)\n",
        "\n",
        "        tok_emb = self.embed_tokens(x) # shape (B, T, n_embed)\n",
        "        logits = self.lm_head(tok_emb) # shape (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        # idx shape (B, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :] # shape(B, T, vocab_size)\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "model = GPT()\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_D9Rqhp52uY",
        "outputId": "e2abe488-dc33-4573-e906-6550fd8d79a0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "  (embed_tokens): Embedding(65, 32)\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Training and Evaluation"
      ],
      "metadata": {
        "id": "asvSFWZ952uY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Evaluation Loop"
      ],
      "metadata": {
        "id": "9WeZR9_E52uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Evaluation Loop\n",
        "#    - Implement the `estimate_loss()` function to evaluate the model on training and validation data.\n",
        "#    - Ensure that the model is in evaluation mode during this process."
      ],
      "metadata": {
        "id": "mN_r7S2c52uY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _ , loss = model(x, y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "H9nuJaeI7pij"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Training Loop"
      ],
      "metadata": {
        "id": "8n3Fg9uK52uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Training Loop\n",
        "#    - Set up the training loop with AdamW optimizer.\n",
        "#    - Include periodic evaluation using `estimate_loss()` and print training/validation losses.\n",
        "#    - Generate sample from the model"
      ],
      "metadata": {
        "id": "S8LBitHR52uY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_iters = 20\n",
        "eval_interval = 500\n",
        "\n",
        "learning_rate=1e-3\n",
        "training_iters=1000\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_iters):\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\" training loss: {losses['train']}, eval loss {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(context)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE-gyThP7rWF",
        "outputId": "a49dbd4a-84df-4929-aa23-a2c54d79154a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " training loss: 4.3456525802612305, eval loss 4.282791614532471\n",
            " training loss: 2.9852001667022705, eval loss 3.005040407180786\n",
            "\n",
            "ZKbt,\n",
            "LNurernd,u wit d werce s abld be beatWos rdy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Enhancing the GPT Model"
      ],
      "metadata": {
        "id": "6dTG8_6O52uY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Model: Positional Embeddings"
      ],
      "metadata": {
        "id": "ytw8kmUp52uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Model: Positional Embeddings\n",
        "#    - Add positional embeddings to the model to capture the order of tokens.\n",
        "#    - Update the forward pass to combine token embeddings with positional embeddings."
      ],
      "metadata": {
        "id": "YIO2eHRm52uZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embed = 32\n",
        "\n",
        "eval_iters = 20\n",
        "eval_interval = 500\n",
        "training_iters=3000\n",
        "learning_rate=1e-3\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, n_embed)\n",
        "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None): # x shape (B, T)\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.embed_tokens(x) # shape (B, T, n_embed)\n",
        "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb;\n",
        "        x = tok_emb\n",
        "\n",
        "        logits = self.lm_head(x) # shape (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        # idx shape (B, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :] # shape(B, T, vocab_size)\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "model = GPT()\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _ , loss = model(x, y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_iters):\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\" training loss: {losses['train']}, eval loss {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(context)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bHuE1eJKsZ6",
        "outputId": "cc0ed6a2-812f-4501-ce41-f1ee8d91ba5b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "  (embed_tokens): Embedding(65, 32)\n",
            "  (pos_embedding_table): Embedding(8, 32)\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            " training loss: 4.314088821411133, eval loss 4.298123359680176\n",
            " training loss: 2.969817638397217, eval loss 2.9305644035339355\n",
            " training loss: 2.705784320831299, eval loss 2.6208765506744385\n",
            " training loss: 2.6057913303375244, eval loss 2.665055990219116\n",
            " training loss: 2.5697872638702393, eval loss 2.577955722808838\n",
            " training loss: 2.5728728771209717, eval loss 2.515190362930298\n",
            "\n",
            "MEathe my myo woucls bed ISIprill-n id co y. teve:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Model: Single Attention Head"
      ],
      "metadata": {
        "id": "0Kdb0R_E52uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Model: Single Attention Head\n",
        "#    - Implement a single attention head within the model to capture token relationships.\n",
        "#    - Incorporate this attention mechanism into the forward pass."
      ],
      "metadata": {
        "id": "mPP540aM52uZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embed = 32\n",
        "head_size = 32\n",
        "\n",
        "eval_iters = 20\n",
        "eval_interval = 500\n",
        "training_iters=3000\n",
        "learning_rate=1e-3\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1)\n",
        "        wei = wei * C**-0.5\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, n_embed)\n",
        "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.attn = Head(head_size)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None): # x shape (B, T)\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.embed_tokens(x) # shape (B, T, n_embed)\n",
        "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb;\n",
        "        x = self.attn(x)\n",
        "\n",
        "        logits = self.lm_head(x) # shape (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        # idx shape (B, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :] # shape(B, T, vocab_size)\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "model = GPT()\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _ , loss = model(x, y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_iters):\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\" training loss: {losses['train']}, eval loss {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(context)[0].tolist()))\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meohg_XnK0Lr",
        "outputId": "95ba3c21-02f3-478f-91d6-119030612912"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "  (embed_tokens): Embedding(65, 32)\n",
            "  (pos_embedding_table): Embedding(8, 32)\n",
            "  (attn): Head(\n",
            "    (query): Linear(in_features=32, out_features=32, bias=False)\n",
            "    (key): Linear(in_features=32, out_features=32, bias=False)\n",
            "    (value): Linear(in_features=32, out_features=32, bias=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "---------------\n",
            " training loss: 4.192816734313965, eval loss 4.181145668029785\n",
            " training loss: 2.9647483825683594, eval loss 2.9163105487823486\n",
            " training loss: 2.6664605140686035, eval loss 2.651362419128418\n",
            " training loss: 2.5779342651367188, eval loss 2.711329698562622\n",
            " training loss: 2.624051570892334, eval loss 2.5673742294311523\n",
            " training loss: 2.556857109069824, eval loss 2.511453866958618\n",
            "\n",
            "---------------\n",
            "\n",
            "FEt, my me tha be lapowoo ove:\n",
            "Mbeastheiin two the\n",
            "\n",
            "---------------\n",
            "\n",
            "Total Parameters: 7553\n",
            "Trainable Parameters: 7553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Model: Multi-Head Attention"
      ],
      "metadata": {
        "id": "gR2duwQk52uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Model: Multi-Head Attention\n",
        "#    - Expand the model to include multiple attention heads.\n",
        "#    - Implement a projection layer to combine the outputs of the multiple heads."
      ],
      "metadata": {
        "id": "Ww8PMvSx52uZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embed = 32\n",
        "head_size = 8\n",
        "n_head = 4\n",
        "\n",
        "eval_iters = 20\n",
        "eval_interval = 500\n",
        "training_iters=3000\n",
        "learning_rate=1e-3\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1)\n",
        "        wei = wei * C**-0.5\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for h in range(n_head)])\n",
        "        self.o_proj = nn.Linear(n_embed, n_embed, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        x = self.o_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, n_embed)\n",
        "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.attn = MultiHeadAttention(n_head, n_embed//n_head)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None): # x shape (B, T)\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.embed_tokens(x) # shape (B, T, n_embed)\n",
        "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb;\n",
        "        x = self.attn(x)\n",
        "\n",
        "        logits = self.lm_head(x) # shape (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        # idx shape (B, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :] # shape(B, T, vocab_size)\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "model = GPT()\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _ , loss = model(x, y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_iters):\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\" training loss: {losses['train']}, eval loss {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(context)[0].tolist()))\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU-VW90CK-U1",
        "outputId": "c848c03e-8dc8-42f8-a2db-e9cdb1f0ac64"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "  (embed_tokens): Embedding(65, 32)\n",
            "  (pos_embedding_table): Embedding(8, 32)\n",
            "  (attn): MultiHeadAttention(\n",
            "    (heads): ModuleList(\n",
            "      (0-3): 4 x Head(\n",
            "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "---------------\n",
            " training loss: 4.190523624420166, eval loss 4.188727378845215\n",
            " training loss: 2.922368288040161, eval loss 2.8527870178222656\n",
            " training loss: 2.746164083480835, eval loss 2.686760187149048\n",
            " training loss: 2.524226427078247, eval loss 2.669847011566162\n",
            " training loss: 2.559569835662842, eval loss 2.566415786743164\n",
            " training loss: 2.5005269050598145, eval loss 2.5069375038146973\n",
            "\n",
            "---------------\n",
            "\n",
            "H:\n",
            "LCBET'AS:\n",
            "Id\n",
            "R Korchoc?\n",
            "zCO fin gom mie dot por\n",
            "\n",
            "---------------\n",
            "\n",
            "Total Parameters: 8577\n",
            "Trainable Parameters: 8577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Building the Transformer Block"
      ],
      "metadata": {
        "id": "PYDs9zxr52uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. Model: MLP"
      ],
      "metadata": {
        "id": "WtoD7b0q52uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Model: MLP\n",
        "#    - Add a multi-layer perceptron (MLP) to the model.\n",
        "#    - The MLP should consist of a projection up, ReLU activation, and a projection down."
      ],
      "metadata": {
        "id": "uHD4X-G152uZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embed = 32\n",
        "head_size = 8\n",
        "n_head = 4\n",
        "\n",
        "eval_iters = 20\n",
        "eval_interval = 500\n",
        "training_iters=3000\n",
        "learning_rate=1e-3\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1)\n",
        "        wei = wei * C**-0.5\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for h in range(n_head)])\n",
        "        self.o_proj = nn.Linear(n_embed, n_embed, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        x = self.o_proj(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.out_proj = nn.Linear(n_embed, 4 * n_embed)\n",
        "        self.in_proj = nn.Linear(4 * n_embed, n_embed)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.out_proj(x)\n",
        "        x = self.act(x)\n",
        "        x = self.in_proj(x)\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, n_embed)\n",
        "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.attn = MultiHeadAttention(n_head, n_embed//n_head)\n",
        "        self.mlp = MLP(n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None): # x shape (B, T)\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.embed_tokens(x) # shape (B, T, n_embed)\n",
        "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb;\n",
        "        x = self.attn(x)\n",
        "        x = self.mlp(x)\n",
        "\n",
        "        logits = self.lm_head(x) # shape (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        # idx shape (B, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :] # shape(B, T, vocab_size)\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "model = GPT()\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _ , loss = model(x, y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_iters):\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\" training loss: {losses['train']}, eval loss {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(context)[0].tolist()))\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKMgZ7ONLIf9",
        "outputId": "e94dcbf0-debe-4ca3-b809-d483d66244af"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "  (embed_tokens): Embedding(65, 32)\n",
            "  (pos_embedding_table): Embedding(8, 32)\n",
            "  (attn): MultiHeadAttention(\n",
            "    (heads): ModuleList(\n",
            "      (0-3): 4 x Head(\n",
            "        (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "        (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
            "  )\n",
            "  (mlp): MLP(\n",
            "    (out_proj): Linear(in_features=32, out_features=128, bias=True)\n",
            "    (in_proj): Linear(in_features=128, out_features=32, bias=True)\n",
            "    (act): ReLU()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "---------------\n",
            " training loss: 4.18421745300293, eval loss 4.177970886230469\n",
            " training loss: 2.92226505279541, eval loss 2.875664472579956\n",
            " training loss: 2.658184289932251, eval loss 2.57914400100708\n",
            " training loss: 2.5772807598114014, eval loss 2.631568431854248\n",
            " training loss: 2.468953847885132, eval loss 2.41440486907959\n",
            " training loss: 2.5281031131744385, eval loss 2.5637919902801514\n",
            "\n",
            "---------------\n",
            "\n",
            "Souts Sive Subamiscin feat dong annd yfat whe tee \n",
            "\n",
            "---------------\n",
            "\n",
            "Total Parameters: 16929\n",
            "Trainable Parameters: 16929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. Model: Transformer Block"
      ],
      "metadata": {
        "id": "Ry15do-u52uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Model: Transformer Block\n",
        "#    - Combine multi-head attention and MLP into a single Transformer block.\n",
        "#    - Use this block in the model to stack multiple layers."
      ],
      "metadata": {
        "id": "52UJ7t8552uZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embed = 32\n",
        "head_size = 8\n",
        "n_head = 4\n",
        "n_layers = 2\n",
        "\n",
        "eval_iters = 20\n",
        "eval_interval = 500\n",
        "training_iters=3000\n",
        "learning_rate=1e-3\n",
        "\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1)\n",
        "        wei = wei * C**-0.5\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "        self.o_proj = nn.Linear(n_head * head_size, n_embed, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        x = self.o_proj(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.up_proj = nn.Linear(n_embed, 4 * n_embed)\n",
        "        self.down_proj = nn.Linear(4 * n_embed, n_embed)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up_proj(x)\n",
        "        x = self.act(x)\n",
        "        x = self.down_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.attn = MultiHeadAttention(n_head, head_size)\n",
        "        self.mlp = MLP(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attn(x)\n",
        "        x = self.mlp(x)\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, n_embed)\n",
        "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.layers = nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layers) ])\n",
        "        # self.layers = nn.Sequential(\n",
        "        #     Block(n_embed, n_head=4),\n",
        "        #     Block(n_embed, n_head=4)\n",
        "        # )\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None): # x shape (B, T)\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.embed_tokens(x) # shape (B, T, n_embed)\n",
        "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb;\n",
        "        x = self.layers(x)\n",
        "        logits = self.lm_head(x) # shape (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        # idx shape (B, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :] # shape(B, T, vocab_size)\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "model = GPT()\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _ , loss = model(x, y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_iters):\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\" training loss: {losses['train']}, eval loss {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(context)[0].tolist()))\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6tqnBlpLNDf",
        "outputId": "371f1b65-11ae-4c21-9f60-c8a4ac3d2801"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "  (embed_tokens): Embedding(65, 32)\n",
            "  (pos_embedding_table): Embedding(8, 32)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (attn): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (up_proj): Linear(in_features=32, out_features=128, bias=True)\n",
            "        (down_proj): Linear(in_features=128, out_features=32, bias=True)\n",
            "        (act): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (1): Block(\n",
            "      (attn): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (up_proj): Linear(in_features=32, out_features=128, bias=True)\n",
            "        (down_proj): Linear(in_features=128, out_features=32, bias=True)\n",
            "        (act): ReLU()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            ")\n",
            "\n",
            "---------------\n",
            " training loss: 4.173075199127197, eval loss 4.180722236633301\n",
            " training loss: 3.229630947113037, eval loss 3.270554780960083\n",
            " training loss: 3.077225923538208, eval loss 3.1028289794921875\n",
            " training loss: 2.937861680984497, eval loss 2.9153316020965576\n",
            " training loss: 2.742178440093994, eval loss 2.70048451423645\n",
            " training loss: 2.591301202774048, eval loss 2.5925872325897217\n",
            "\n",
            "---------------\n",
            "\n",
            " ay oihes doulmom thinss h hat ry hean et eul, str\n",
            "\n",
            "---------------\n",
            "\n",
            "Total Parameters: 29377\n",
            "Trainable Parameters: 29377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Final Enhancements"
      ],
      "metadata": {
        "id": "E-e9Tblq52uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. Model: Skip connections, normalization and dropout"
      ],
      "metadata": {
        "id": "OB9qent_52uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Model: Skip Connections\n",
        "#    - Implement skip connections (residual connections) around the attention and MLP layers.\n",
        "#    - Add layer normalization before applying the skip connections to stabilize training.\n",
        "#    - Include dropout layers in both the attention and MLP layers to prevent overfitting."
      ],
      "metadata": {
        "id": "PF9xB5pu52ua"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embed = 32\n",
        "head_size = 8\n",
        "n_head = 4\n",
        "n_layers = 2\n",
        "\n",
        "dropout=0.1\n",
        "\n",
        "eval_iters = 20\n",
        "eval_interval = 500\n",
        "training_iters=3000\n",
        "learning_rate=1e-3\n",
        "\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1)\n",
        "        wei = wei * C**-0.5\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "        self.o_proj = nn.Linear(n_head * head_size, n_embed, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        x = self.o_proj(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.up_proj = nn.Linear(n_embed, 4 * n_embed)\n",
        "        self.down_proj = nn.Linear(4 * n_embed, n_embed)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up_proj(x)\n",
        "        x = self.act(x)\n",
        "        x = self.down_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.attn = MultiHeadAttention(n_head, head_size)\n",
        "        self.mlp = MLP(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dropout(self.attn(self.ln1(x)))\n",
        "        x = x + self.dropout(self.mlp(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, n_embed)\n",
        "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.layers = nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layers) ])\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x, targets=None): # x shape (B, T)\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.embed_tokens(x) # shape (B, T, n_embed)\n",
        "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb;\n",
        "        x = self.layers(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x) # shape (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        # idx shape (B, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :] # shape(B, T, vocab_size)\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "model = GPT()\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _ , loss = model(x, y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_iters):\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\" training loss: {losses['train']}, eval loss {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(context)[0].tolist()))\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7DcHypvLdzj",
        "outputId": "d1572c5f-deff-48d2-fa5d-f88556a1bd5d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT(\n",
            "  (embed_tokens): Embedding(65, 32)\n",
            "  (pos_embedding_table): Embedding(8, 32)\n",
            "  (layers): Sequential(\n",
            "    (0): Block(\n",
            "      (attn): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (up_proj): Linear(in_features=32, out_features=128, bias=True)\n",
            "        (down_proj): Linear(in_features=128, out_features=32, bias=True)\n",
            "        (act): ReLU()\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (attn): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-3): 4 x Head(\n",
            "            (query): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (key): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (value): Linear(in_features=32, out_features=8, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (up_proj): Linear(in_features=32, out_features=128, bias=True)\n",
            "        (down_proj): Linear(in_features=128, out_features=32, bias=True)\n",
            "        (act): ReLU()\n",
            "      )\n",
            "      (ln1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=32, out_features=65, bias=True)\n",
            "  (ln_f): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "\n",
            "---------------\n",
            " training loss: 4.373117923736572, eval loss 4.365534782409668\n",
            " training loss: 2.8125882148742676, eval loss 2.675476551055908\n",
            " training loss: 2.5230605602264404, eval loss 2.6043498516082764\n",
            " training loss: 2.4970192909240723, eval loss 2.5364904403686523\n",
            " training loss: 2.385586977005005, eval loss 2.427258014678955\n",
            " training loss: 2.4151623249053955, eval loss 2.323061943054199\n",
            "\n",
            "---------------\n",
            "\n",
            "feald ?o thoUrd yourflat he reimsce, emy an his wa\n",
            "\n",
            "---------------\n",
            "\n",
            "Total Parameters: 29697\n",
            "Trainable Parameters: 29697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7: Final Training and Evaluation"
      ],
      "metadata": {
        "id": "5PsvXR8b52ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. Final Evaluation and Text Generation"
      ],
      "metadata": {
        "id": "mVvATyTD52ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. Final Evaluation and Text Generation\n",
        "#    - Train the final GPT model with the full architecture.\n",
        "#    - Evaluate the final model on the validation set.\n",
        "#    - Use the trained model to generate new text samples."
      ],
      "metadata": {
        "id": "spm9k-Nd52ua"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embed = 32\n",
        "head_size = 8\n",
        "n_head = 4\n",
        "n_layers = 4\n",
        "\n",
        "dropout=0.1\n",
        "\n",
        "eval_iters = 20\n",
        "eval_interval = 1000\n",
        "training_iters=10000\n",
        "learning_rate=1e-3\n",
        "\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1)\n",
        "        wei = wei * C**-0.5\n",
        "        wei = torch.masked_fill(wei, self.tril[:T, :T]==0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "        self.o_proj = nn.Linear(n_head * head_size, n_embed, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        x = self.o_proj(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.up_proj = nn.Linear(n_embed, 4 * n_embed)\n",
        "        self.down_proj = nn.Linear(4 * n_embed, n_embed)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up_proj(x)\n",
        "        x = self.act(x)\n",
        "        x = self.down_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.attn = MultiHeadAttention(n_head, head_size)\n",
        "        self.mlp = MLP(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dropout(self.attn(self.ln1(x)))\n",
        "        x = x + self.dropout(self.mlp(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(vocab_size, n_embed)\n",
        "        self.pos_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.layers = nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layers) ])\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x, targets=None): # x shape (B, T)\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.embed_tokens(x) # shape (B, T, n_embed)\n",
        "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb;\n",
        "        x = self.layers(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x) # shape (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B*T)\n",
        "\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=50):\n",
        "        # idx shape (B, T)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :] # shape(B, T, vocab_size)\n",
        "            probs = F.softmax(logits,dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "model = GPT()\n",
        "model = model.to(device)\n",
        "\n",
        "# Calculate and print the number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'\\nTotal Parameters: {total_params}')\n",
        "print(f'Trainable Parameters: {trainable_params}')\n",
        "\n",
        "print('\\n---------------')\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.ones(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            _ , loss = model(x, y)\n",
        "            losses[k] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_iters):\n",
        "\n",
        "    if i % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\" training loss: {losses['train']}, eval loss {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k81nSMh3OK64",
        "outputId": "8371c843-038e-4dec-ebdb-8b77b49af23c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total Parameters: 54849\n",
            "Trainable Parameters: 54849\n",
            "\n",
            "---------------\n",
            " training loss: 4.370202541351318, eval loss 4.357987880706787\n",
            " training loss: 2.5559301376342773, eval loss 2.5146987438201904\n",
            " training loss: 2.4333717823028564, eval loss 2.3698134422302246\n",
            " training loss: 2.2964673042297363, eval loss 2.388180732727051\n",
            " training loss: 2.3420510292053223, eval loss 2.3240602016448975\n",
            " training loss: 2.209441900253296, eval loss 2.3096933364868164\n",
            " training loss: 2.253512144088745, eval loss 2.3528244495391846\n",
            " training loss: 2.2819905281066895, eval loss 2.295457124710083\n",
            " training loss: 2.216463327407837, eval loss 2.1905064582824707\n",
            " training loss: 2.2447941303253174, eval loss 2.06135892868042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_iters = 1000\n",
        "final_losses = estimate_loss()\n",
        "print(f\"\\nFinal training loss: {final_losses['train']}\")\n",
        "print(f\"Final validation loss: {final_losses['val']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A5XmIQqTSpa",
        "outputId": "267fb402-8001-4116-9620-13455d173fdf"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final training loss: 2.1516566276550293\n",
            "Final validation loss: 2.198732614517212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some text to see the model in action\n",
        "context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "generated_text = decode(model.generate(context, max_new_tokens=5000)[0].tolist())\n",
        "\n",
        "print(\"\\nGenerated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaKPeckDREiF",
        "outputId": "da9cbdf6-c9f9-4ab3-9d10-5e7b5d30d5d2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:\n",
            "\n",
            "What; my nos to me for toes silg mesar, lord thou b'd 'ceece,\n",
            "And and berew'sermars of chiss us in hom thant bles mome of ipare!\n",
            "\n",
            "Endosir,\n",
            "Frelt his loideng of whall I wedch tus tohere. Vareing, coeadem, your Of sodsoankes,\n",
            "Wom it:\n",
            "Way my is noebe?\n",
            "\n",
            "AUfor Bastaren of to mine\n",
            "Fe dratelf ofer slaight of whoin The hingh:\n",
            "Tot whenk mingr had he et it, if hingaiunst, es somenpe; boren, I he lat o' wits thim now ith fe the I have saygrean, h yere thou at will has furing herds;\n",
            "Ting; He hart notu\n",
            "Whirs whill etheeteont.\n",
            " and he pupckel artrow to suers, yey\n",
            "ANd I y OUMARDEN:\n",
            "Youth and spoange I mere sis on on heeavaod,\n",
            "That curnestingen ? Wothem I, you marcoobut the a ste hen so whall but; gode;\n",
            "Roventinlen supee laeepe Leveacul Ask walut Vens upard:\n",
            "Wik, hion And't hades noun:\n",
            "y her le, it that in your Yourece, on my would a demenast;\n",
            "Senou here wilth trage, jestic? hay se me for res: ain, ucoO:\n",
            "Lor fat\n",
            "ith have him; town metay gor, dord; beartis liawit,\n",
            "I comy, fut thee:\n",
            "Aus the have il\n",
            "Andour cortife orn biveinges laar theare usea in thy seromcond, orke tons\n",
            "Coser home wernd and\n",
            "ID there til Artruess you, her ans of heas lovee hestrowesers a,\n",
            "Yet lit,\n",
            "Tin. mone:\n",
            "The Rlerake!\n",
            "Thos mincond of rut!\n",
            "\n",
            "DUlY Wit;\n",
            "Vonf thallo ald a\n",
            "Fristiorineve, as whrooncey.\n",
            "\n",
            "In:\n",
            "Cranaod:\n",
            "And Whoucrothe shalEcloucen; mut Row it kich therbrerban bit,\n",
            "Buckices ton this ent ace! theaw, my\n",
            "Brees one,\n",
            "And hat are\n",
            "Tee ow;\n",
            "Droblahan kitong the cow;\n",
            "K:\n",
            "We les kelll,\n",
            "Adres if,\n",
            "Buech Dothest Rat thes sut of mess? Mreece whieng have and ingruttedereb fest itmins chaer hir:\n",
            "I his we liss,y; on I thd wister to frisretes caisssese lemastooog,\n",
            "Sice joatter dive;\n",
            "To heneres faterse's hamy that pomoos te thas:\n",
            "The somen\n",
            "Herm Buied your;\n",
            "Fives shis my for I mener bardan it beint myme ofeat ughat\n",
            "enge, and the powat me thoues ton fore;\n",
            "Buthy frome:\n",
            "Whall.\n",
            "\n",
            "Fringeds oboow ceIfer, now hessid pert thate;\n",
            "For our hats is shels is dy flare when Peay trow hergor in engepur are I what us fripore;\n",
            "O ot\n",
            "Ank hes mor wed zaid letsle;\n",
            "Thoare sestat lostet.\n",
            "Wo; Ray, fos it in whate alt on thre ifer, hay your fusepwornes dap thave.\n",
            "Wh me the fors's putm yipher: if ther lers\n",
            "Oy I there ratheey a whath nien, fory of hem wall theres I uph son ons,, I woid snd and mom in en the home, beeler blo, is fore is deabdlore hour, that by to it haver, her rueds as,\n",
            "In seren\n",
            " tore naes mine\n",
            "The he. 'lord;\n",
            "OMEdsenk, in rinkbe\n",
            "Bet, aly that! demereics, have sud;\n",
            "Bake Sever usim Patuo'loo;\n",
            "Metink.  xigr;\n",
            "With, me poufor ler in our weltn aghth sen you to I when I hiss of bew';\n",
            "An ur\n",
            "A here o'ser, hit mey mertes won buty band is muntese bieerer is of eser wI cork I wher the it, ibe Gon\n",
            "Thes on unest oser, and youy!\n",
            "Thee theam:\n",
            "Pantafods edencous-\n",
            "\n",
            "Floveht pow,\n",
            "I kn doa, roth.\n",
            "\n",
            "Do of in un foresfe; a,longe son, me bies:\n",
            "Wit hours, tist, thoull, wife hus, fer on the to grostseld of patep?\n",
            "\n",
            "BOBut yet?\n",
            "To me post his peeet Todem, cualter?\n",
            "\n",
            "But to ut knous:\n",
            "What is entere coverd youdur prinentes re thvis has;\n",
            "Foor nes bake? Jere art my gee, Sirterm.\n",
            "\n",
            "LUENORDUMIZHN: whell;\n",
            "Thit.\n",
            "Fon hoost on lest! my rue thus hald'd deyournow wall, shou, Itn, as dor thy paie, staI rices,\n",
            "Wen cird s'stay, the a sorst I fcuged ecitmen it my le thiire o. Bown, this so to.\n",
            "Cen:\n",
            "But men Hat he:\n",
            "Weros're?\n",
            "\n",
            "And shid\n",
            "Too warst ain n Edisterche he fous merit ketiourghins Set pustoof for the coves af worentre son id s and kis, well as ben's and goise eroun Jences the\n",
            "bre of an, yowY\n",
            "This said aygursar lork.\n",
            "\n",
            "After Mrardymuer.\n",
            "\n",
            "Wherveak that:\n",
            "Fell Buth fich is thee your in et mose die I whar and, ipus,\n",
            "Dim Marececke iantion\n",
            "A drus sim stroserard cond nor for of bleblightein econ hornes in shat yoSeg, to feattuo o ker ure yous cancks then me tof;\n",
            "The bnokes:\n",
            "Woun ick opera wers:\n",
            "Unsiven, noge not tio be-moten wick\n",
            "Come her forth hLos; fard in the pathhringe fear lar deamaght in t,\n",
            "Nos paillates at well colges ne he lan hee!\n",
            "Dot\n",
            "Au kn.\n",
            "TIOUK:\n",
            "Bit it Mitece\n",
            "ind us lell us my in, kofelown! the ther for yo sere.\n",
            ":\n",
            "For as hatw.\n",
            "\n",
            "Soqut, Angrountiin fuselir!\n",
            "Es sour I mere no hefus cord and ing form yound Peroweie condarart itiver, fornver ban. \n",
            "Goy, Lecerce vemome, dost Poy\n",
            "Whis havisicinnp marel otheull uscums me hisagw er nome fere adwis cUSetequtl for news I ses not ere beles traincen hat om that compleed nelk ars has lared, yours And wermaur:\n",
            "Watar!\n",
            "\n",
            "STHO, I:\n",
            "Uhwantio gursce ideked\n",
            "And ye whare ence bucooth hertighis hoRes I his I wham thee bunt io arre rom, I ther, to his tond a fen astat I the his will il for is to susecenied,\n",
            "Mown any I dorer:\n",
            "The hare.\n",
            "\n",
            "DUAIUak beayOerie mackerserss,\n",
            "And heme hee trib mror of mranty is bloonvesth!\n",
            "\n",
            "jit corY, of thes shy wisull?\n",
            "For is thoe olve this red, Pron of this I sot.\n",
            "\n",
            "TYIUCUEND:\n",
            "O, met inber'd\n",
            "in best taring brer muck and bente O to what;\n",
            "s to buts clar?\n",
            "W, do ouet arten eaim, herecowar I him mutth et he, doust, mitespum to anreee: sasee.\n",
            "\n",
            "LUCIY OLIBEL:\n",
            "\n",
            "Prokin jove to gicto to wholks the hare to heal\n",
            "I weremang bat anderer there a callersle to ten war fen were pold thou dour\n",
            "S\n"
          ]
        }
      ]
    }
  ]
}