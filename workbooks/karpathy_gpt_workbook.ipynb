{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IGKHI92FoakW",
        "Xk8VM8mAoakW",
        "3LRCJO8ucpFX"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Workbook Overview\n",
        "\n",
        "This workbook guides you through coding a Transformer-based GPT model from scratch. It is based on Andrej Karpathy's \"Zero to Hero\" YouTube tutorial titled [\"Let's build GPT: from scratch, in code, spelled out\"](https://www.youtube.com/watch?v=kCc8FmEb1nY).\n",
        "\n",
        "### Who Is This For?\n",
        "\n",
        "- Those familiar with Andrej Karpathy’s \"Zero to Hero\" series and looking to code GPT from scratch.\n",
        "\n",
        "### How to Use This Workbook\n",
        "\n",
        "1. **Create a Copy**: To begin, you'll need your own copy of this Colab workbook.\n",
        "   - In Colab, go to **File** > **Save a copy in Drive**.\n",
        "   - Rename the file if you'd like.\n",
        "   - You are now ready to start coding.\n",
        "\n",
        "2. **Workbook Structure**:\n",
        "   - The workbook is divided into three sections:\n",
        "     - **Coding Instructions**: Detailed steps for building key GPT model components.\n",
        "     - **Coding Exercises**: Implement what you've learned as you follow along.\n",
        "     - **Code Solutions**: Check your work or get unstuck by reviewing complete solutions.\n",
        "   - Use the Table of Contents on the left sidebar for easy navigation.\n",
        "\n",
        "\n",
        "\n",
        "### Purpose\n",
        "\n",
        "- This workbook aims to help you understand the construction of Transformer-based language models like GPT, from the ground up.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SrG7cmREoakV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List of instructions"
      ],
      "metadata": {
        "id": "IGKHI92FoakW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Let's Build GPT (Step-by-Step Transformer Language Model)\n",
        "\n",
        "## Part 1: Setup and Data Preparation\n",
        "\n",
        "# 1. Imports & Configurations\n",
        "#    - Import necessary libraries (`torch`, `torch.nn`, `torch.nn.functional`, etc.).\n",
        "#    - Set device configuration (CPU/GPU).\n",
        "#    - Set random seed for reproducibility.\n",
        "\n",
        "# 2. Download Dataset\n",
        "#    - Download and load the \"tinyshakespeare\" dataset.\n",
        "#    - https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "#    - Read the text data from the file.\n",
        "\n",
        "# 3. Vocabulary Creation\n",
        "#    - Extract unique characters from the dataset.\n",
        "#    - Determine the vocabulary size.\n",
        "\n",
        "# 4. Tokenizer\n",
        "#    - Create mappings for character-to-index (`stoi`) and index-to-character (`itos`).\n",
        "#    - Implement `encode()` and `decode()` functions for tokenization.\n",
        "\n",
        "# 5. Train and Test Splits\n",
        "#    - Convert the entire dataset into token indices.\n",
        "#    - Split the data into training and validation sets (90/10 split).\n",
        "\n",
        "# 6. Dataloader\n",
        "#    - Define a function `get_batch()` to generate batches of data for training and evaluation.\n",
        "#    - Ensure that each batch contains sequences of fixed block size.\n",
        "\n",
        "## Part 2: Building the Initial GPT Model\n",
        "\n",
        "# 7. Model: Embedding Layer, Generate Function, Training Loop\n",
        "#    - Create a dataclass `GPTConfig` to store model hyperparameters.\n",
        "#    - Implement the GPT class with an embedding layer, producing logits for output.\n",
        "#    - Implement the `generate()` function to generate text using the trained model.\n",
        "#    - Instantiate a model.\n",
        "#    - Print the number of trainable parameters.\n",
        "#    - Pass one minibatch through the model.\n",
        "#    - Generate a sample from the model (output should be garbled/random initially).\n",
        "\n",
        "# 8. Model: Training Loop\n",
        "#    - Set up the training loop with AdamW optimizer.\n",
        "#    - block_size:8, Batch size: 32, Training steps: 3,000, learning_rate: 1e-2\n",
        "#    - Generate sample from the model\n",
        "#    - Model output should have more structure after training\n",
        "\n",
        "# 9. Evaluation Loop\n",
        "#    - Implement the `estimate_loss()` function to evaluate the model on training and validation data.\n",
        "#    - Ensure that the model is in evaluation mode during this process.\n",
        "#    - Set up the training loop as before.\n",
        "#    - Include periodic evaluation using `estimate_loss()` and print 10 training/validation losses.\n",
        "#    - Generate sample from the model.\n",
        "\n",
        "## Part 3: Enhancing the GPT Model\n",
        "\n",
        "# 10. Model: Positional Embeddings\n",
        "#    - Add positional embeddings to the model.\n",
        "#    - Train, evaluate and generate sample from the model.\n",
        "\n",
        "# 11. Model: Single Attention Head\n",
        "#    - Implement a single attention head.\n",
        "#    - Train, evaluate and generate sample from the model.\n",
        "\n",
        "# 12. Model: Multi-Head Attention\n",
        "#    - Expand the model to include multiple attention heads.\n",
        "#    - Implement a projection layer to combine the outputs of the multiple heads.\n",
        "#    - Train, evaluate and generate sample from the model.\n",
        "\n",
        "## Part 4: Building the Transformer Block\n",
        "\n",
        "# 13. Model: MLP\n",
        "#    - Add a multi-layer perceptron (MLP) to the model.\n",
        "#    - The MLP should consist of a projection up, ReLU activation, and a projection down.\n",
        "\n",
        "# 14. Model: Transformer Block\n",
        "#    - Combine multi-head attention and MLP into a single Transformer block.\n",
        "\n",
        "## Part 5: Final Enhancements\n",
        "\n",
        "# 15. Model: Skip Connections, normalization and dropout\n",
        "#    - Implement skip connections (residual connections) around the attention and MLP layers.\n",
        "#    - Add layer normalization before applying the skip connections to stabilize training.\n",
        "#    - Include dropout layers in both the attention and MLP layers to prevent overfitting.\n",
        "#    - Implement two or more Blocks.\n",
        "\n",
        "# 16. Model: MultiHeadAttention [Alternate Implementation] (Optional)\n",
        "#    - Implement MultiHeadAttention by utilizing a single class.\n",
        "#    - Refer to the following resource for guidance:\n",
        "#      https://github.com/rasbt/LLMs-from-scratch/tree/main/ch03/02_bonus_efficient-multihead-attention\n",
        "\n",
        "## Part 6: Final Training and Evaluation\n",
        "\n",
        "# 18. Final Evaluation and Text Generation\n",
        "#    - Train the final GPT model with the complete architecture on a GPU for improved performance.\n",
        "#    - Model hyperparameters:\n",
        "#        - block_size: 256\n",
        "#        - n_embd: 128\n",
        "#        - n_head: 6\n",
        "#        - n_layer: 6\n",
        "#        - head_size: 16\n",
        "#    - Training hyperparameters:\n",
        "#        - batch_size: 128\n",
        "#        - max_iters: 5000\n",
        "#        - learning_rate: 1e-3\n",
        "#        - eval_interval: 500\n",
        "#        - eval_iters: 100\n",
        "#    - Evaluate the final model on the validation set at regular intervals.\n",
        "#    - Use the trained model to generate new text samples and assess its performance."
      ],
      "metadata": {
        "id": "kJHON7amaZ9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding Exercises"
      ],
      "metadata": {
        "id": "Xk8VM8mAoakW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setup and Data Preparation"
      ],
      "metadata": {
        "id": "l0sproQYoakW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Imports & Configurations"
      ],
      "metadata": {
        "id": "Ltu57k9poakW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports & Configurations\n",
        "#    - Import necessary libraries (`torch`, `torch.nn`, `torch.nn.functional`, etc.).\n",
        "#    - Set device configuration (CPU/GPU).\n",
        "#    - Set random seed for reproducibility.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9840161-cc5c-4258-e529-8caafa783399",
        "id": "McXGFvijoakW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Download Dataset"
      ],
      "metadata": {
        "id": "wSsgeibqoakX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Download Dataset\n",
        "#    - Download and load the \"tinyshakespeare\" dataset.\n",
        "#    - https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "#    - Read the text data from the file.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abdc1763-1094-4d54-e76d-d1bf138a4e5f",
        "id": "-eUW7mCaoakX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-23 12:18:48--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-08-23 12:18:48 (16.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Vocabulary Creation"
      ],
      "metadata": {
        "id": "ZahFDIHVoakX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Vocabulary Creation\n",
        "#    - Extract unique characters from the dataset.\n",
        "#    - Determine the vocabulary size.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "z9zmckUOoakX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Tokenizer"
      ],
      "metadata": {
        "id": "JKLKf0B7oakX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Tokenizer\n",
        "#    - Create mappings for character-to-index (`stoi`) and index-to-character (`itos`).\n",
        "#    - Implement `encode()` and `decode()` functions for tokenization.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "474042a4-dd17-473d-b5a8-7ed0ae4f9cf8",
        "id": "l6fbfYrkoakX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Train and Test Splits"
      ],
      "metadata": {
        "id": "Ar6KaUeQoakX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Train and Test Splits\n",
        "#    - Convert the entire dataset into token indices.\n",
        "#    - Split the data into training and validation sets (90/10 split).\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b758f1f8-9c2d-46db-f7ee-2f21a1cb84bf",
        "id": "tKFGfWq-oakX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1003854 111540\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n",
            "tensor([12,  0,  0, 19, 30, 17, 25, 21, 27, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Dataloader"
      ],
      "metadata": {
        "id": "yM2se7qKoakY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Dataloader\n",
        "#    - Define a function `get_batch()` to generate batches of data for training and evaluation.\n",
        "#    - Ensure that each batch contains sequences of fixed block size.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90062c1d-58f2-4d6c-db4a-0120acfc87e3",
        "id": "LEjIBG73oakY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
              "         [44, 53, 56,  1, 58, 46, 39, 58],\n",
              "         [52, 58,  1, 58, 46, 39, 58,  1],\n",
              "         [25, 17, 27, 10,  0, 21,  1, 54]]),\n",
              " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
              "         [53, 56,  1, 58, 46, 39, 58,  1],\n",
              "         [58,  1, 58, 46, 39, 58,  1, 46],\n",
              "         [17, 27, 10,  0, 21,  1, 54, 39]]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Building the Initial GPT Model"
      ],
      "metadata": {
        "id": "hZ7JkUjSoakY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Model: Embedding Layer, Generate Function, Training Loop"
      ],
      "metadata": {
        "id": "tos3dPPGoakY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Consolidation\n",
        "# - Starting from this cell, we'll include all the code from 'Part 1: Setup and Data Preparation'\n",
        "#   at the beginning of each subsequent code cell. This approach ensures that we have a complete\n",
        "#   and up-to-date version of the entire codebase as we incrementally build the Transformer model.\n",
        "# - In addition to the data preparation code, we'll also include the model hyperparameters\n",
        "#   and training configurations in each cell.\n",
        "# - Going forward, always copy the complete code from the previous cell into the next one,\n",
        "#   and then add new features or enhancements. This method keeps the development process clear,\n",
        "#   and makes it easier to track progress and make modifications.\n",
        "# - This is the first cell where we begin consolidating code, so ensure that you bring over all\n",
        "#   necessary components from the previous steps as we continue to build on the Transformer.\n",
        "\n",
        "# 7. Model: Embedding Layer, Generate Function, Training Loop\n",
        "#    - Create a dataclass `GPTConfig` to store model hyperparameters.\n",
        "#    - Implement the GPT class with an embedding layer, producing logits for output.\n",
        "#    - Implement the `generate()` function to generate text using the trained model.\n",
        "#    - Instantiate a model.\n",
        "#    - Print the number of trainable parameters.\n",
        "#    - Pass one minibatch through the model.\n",
        "#    - Generate a sample from the model (output should be garbled/random initially).\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dc7c7f8-104b-4083-e0de-36ca6e7cf3d1",
        "id": "EPv815JMoakY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 4225\n",
            "Trainable Parameters: 4225\n",
            "\n",
            "P-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3!dcb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Model: Training Loop"
      ],
      "metadata": {
        "id": "UM4MWCOAoakY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Model: Training Loop\n",
        "#    - Set up the training loop with AdamW optimizer.\n",
        "#    - block_size:8, Batch size: 32, Training steps: 3,000, learning_rate: 1e-2\n",
        "#    - Generate sample from the model\n",
        "#    - Model output should have more structure after training\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c69e9125-501f-42d2-a737-f5775ca1ee6b",
        "id": "kl8fwUiEoakY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 4225\n",
            "Trainable Parameters: 4225\n",
            "loss: 2.5201265811920166\n",
            "\n",
            "thest, thavor 'spe-d th.\n",
            "\n",
            "O d g oow.\n",
            "Gosul d lllarclldd Bush auach nd t hethind he s wntatesi: slou \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Evaluation Loop"
      ],
      "metadata": {
        "id": "wbsIx-qUoakY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Evaluation Loop\n",
        "#    - Implement the `estimate_loss()` function to evaluate the model on training and validation data.\n",
        "#    - Ensure that the model is in evaluation mode during this process.\n",
        "#    - Set up the training loop as before.\n",
        "#    - Include periodic evaluation using `estimate_loss()` and print 10 training/validation losses.\n",
        "#    - Generate sample from the model.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15a9edb6-8fd7-4fd2-ca52-6eeb31c8cdde",
        "id": "Ngxm4DMtoakY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 4225\n",
            "Trainable Parameters: 4225\n",
            "Training loss: 4.733225345611572, Validation loss: 4.725981712341309\n",
            "Training loss: 2.7876458168029785, Validation loss: 2.815702438354492\n",
            "Training loss: 2.5510098934173584, Validation loss: 2.5644922256469727\n",
            "Training loss: 2.491964340209961, Validation loss: 2.507617950439453\n",
            "Training loss: 2.4817771911621094, Validation loss: 2.51529860496521\n",
            "Training loss: 2.471799850463867, Validation loss: 2.4981491565704346\n",
            "Training loss: 2.487903594970703, Validation loss: 2.495173454284668\n",
            "Training loss: 2.4741978645324707, Validation loss: 2.504298210144043\n",
            "Training loss: 2.4576830863952637, Validation loss: 2.494094133377075\n",
            "Training loss: 2.448154926300049, Validation loss: 2.496605157852173\n",
            "\n",
            "Antat p OLLal kengapGHAnd mainateal LO dourd Rimou t t seise:\n",
            "PWhomo.\n",
            "Torilik f s wo pe kear thinewie; y, ndunk smeaithe osmullly\n",
            "Titouinch dgectonc eafithoutitay\n",
            "INomp hyetherour:\n",
            "ME:\n",
            "\n",
            "Cors pres'se g\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Enhancing the GPT Model"
      ],
      "metadata": {
        "id": "eJlH-tncoakY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Model: Positional Embeddings"
      ],
      "metadata": {
        "id": "D13zbgJPoakZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Model: Positional Embeddings\n",
        "#    - Add positional embeddings to the model.\n",
        "#    - Train, evaluate and generate sample from the model.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "960067f9-46b9-4d70-ed55-be1a6539d376",
        "id": "z9pPj9LBoakZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 4481\n",
            "Trainable Parameters: 4481\n",
            "Training loss: 4.474524974822998, Validation loss: 4.480035305023193\n",
            "Training loss: 2.522061347961426, Validation loss: 2.5282061100006104\n",
            "Training loss: 2.5352325439453125, Validation loss: 2.5343899726867676\n",
            "Training loss: 2.4970154762268066, Validation loss: 2.5091142654418945\n",
            "Training loss: 2.4937195777893066, Validation loss: 2.5417206287384033\n",
            "Training loss: 2.511420965194702, Validation loss: 2.5166428089141846\n",
            "Training loss: 2.5183358192443848, Validation loss: 2.521740198135376\n",
            "Training loss: 2.511129140853882, Validation loss: 2.526968002319336\n",
            "Training loss: 2.483370304107666, Validation loss: 2.5209450721740723\n",
            "Training loss: 2.472799062728882, Validation loss: 2.5132687091827393\n",
            "\n",
            "Wim anshlar, d, kn:\n",
            "Who howg.\n",
            "Theabjor ge?\n",
            "ANonyof thenthard;\n",
            "zerig he,\n",
            "fondr byo maty ind:\n",
            "NG tt t chy me on Whorssur on\n",
            "L: ce liceyo cen,\n",
            "Sst rend, seremy he.\n",
            "' ar hofr our br.\n",
            "ANEOXFiave\n",
            "BEnour CH:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Model: Single Attention Head"
      ],
      "metadata": {
        "id": "CsD_7HyeoakZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Model: Single Attention Head\n",
        "#    - Implement a single attention head.\n",
        "#    - Train, evaluate and generate sample from the model.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fbc0769-89f8-4f2f-f429-2c26452cfffb",
        "id": "ltIhiCZloakZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 7553\n",
            "Trainable Parameters: 7553\n",
            "Training loss: 4.196639537811279, Validation loss: 4.200407981872559\n",
            "Training loss: 2.4888217449188232, Validation loss: 2.533363103866577\n",
            "Training loss: 2.463995933532715, Validation loss: 2.487450361251831\n",
            "Training loss: 2.433098077774048, Validation loss: 2.476203203201294\n",
            "Training loss: 2.4688870906829834, Validation loss: 2.4878599643707275\n",
            "Training loss: 2.41792631149292, Validation loss: 2.457453727722168\n",
            "Training loss: 2.422886610031128, Validation loss: 2.4499640464782715\n",
            "Training loss: 2.4246163368225098, Validation loss: 2.436007499694824\n",
            "Training loss: 2.389042615890503, Validation loss: 2.4294393062591553\n",
            "Training loss: 2.425231695175171, Validation loss: 2.405696153640747\n",
            "\n",
            "\n",
            "Wha thanceith belo ISIZETe can\n",
            "CALEROGSOG:\n",
            "The! highs a ckitr, an isprinen kst I bast anto le wilt conto;\n",
            "Waf doreay rssand sag cyor thapcols ar re kimer hererd hadi heealer ulcove.\n",
            "\n",
            "The yst;\n",
            "G ESS:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Model: Multi-Head Attention"
      ],
      "metadata": {
        "id": "CPUgiB4VoakZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Model: Multi-Head Attention\n",
        "#    - Expand the model to include multiple attention heads.\n",
        "#    - Implement a projection layer to combine the outputs of the multiple heads.\n",
        "#    - Train, evaluate and generate sample from the model.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6acc2683-6d32-4315-aaee-11d1ac3b58e5",
        "id": "mYSKMmVvoakZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 8609\n",
            "Trainable Parameters: 8609\n",
            "Training loss: 4.203001022338867, Validation loss: 4.203583240509033\n",
            "Training loss: 2.423476219177246, Validation loss: 2.4322385787963867\n",
            "Training loss: 2.3412065505981445, Validation loss: 2.3748598098754883\n",
            "Training loss: 2.332437038421631, Validation loss: 2.366330862045288\n",
            "Training loss: 2.2979369163513184, Validation loss: 2.339280605316162\n",
            "Training loss: 2.2743730545043945, Validation loss: 2.3021461963653564\n",
            "Training loss: 2.2756242752075195, Validation loss: 2.305638313293457\n",
            "Training loss: 2.258408546447754, Validation loss: 2.2880289554595947\n",
            "Training loss: 2.218405246734619, Validation loss: 2.2747461795806885\n",
            "Training loss: 2.219954490661621, Validation loss: 2.2759642601013184\n",
            "\n",
            "Thing gock,\n",
            "As dry orgoord\n",
            "YICORIA:\n",
            "And prit, eea If sacdek? \n",
            "mact all prer nold?\n",
            "What, chat?\n",
            "\n",
            "Whill\n",
            "Onll,\n",
            "Then;\n",
            "To grarn bew; an as to milesen we par dights womy\n",
            "Whal, sth,\n",
            "Drens, Efage, whatt yane w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Building the Transformer Block"
      ],
      "metadata": {
        "id": "MNmFSwJyoakZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. Model: MLP"
      ],
      "metadata": {
        "id": "LWHrbcHkoakZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Model: MLP\n",
        "#    - Add a multi-layer perceptron (MLP) to the model.\n",
        "#    - The MLP should consist of a projection up, ReLU activation, and a projection down.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "099b4262-0176-49ce-e032-c86b2d67ea5a",
        "id": "i-pIFYDwoakZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 16961\n",
            "Trainable Parameters: 16961\n",
            "Training loss: 4.153048992156982, Validation loss: 4.156349182128906\n",
            "Training loss: 2.447960615158081, Validation loss: 2.4581236839294434\n",
            "Training loss: 2.33398699760437, Validation loss: 2.3294832706451416\n",
            "Training loss: 2.2790815830230713, Validation loss: 2.3204283714294434\n",
            "Training loss: 2.2687125205993652, Validation loss: 2.312082290649414\n",
            "Training loss: 2.2480599880218506, Validation loss: 2.2877395153045654\n",
            "Training loss: 2.2398452758789062, Validation loss: 2.2740907669067383\n",
            "Training loss: 2.216015577316284, Validation loss: 2.2861952781677246\n",
            "Training loss: 2.192251205444336, Validation loss: 2.265817642211914\n",
            "Training loss: 2.1534619331359863, Validation loss: 2.255052328109741\n",
            "\n",
            "You to goodamby earsabluth say an heak no I torlans nothen ack, witin owouir to mee\n",
            "To for you an in there deart, vour tormesten as Row\n",
            "Must caill sher't shiciveme? dold gues,\n",
            "Histle,\n",
            "Ray nosur gaid t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. Model: Transformer Block"
      ],
      "metadata": {
        "id": "YIsVcBayoakZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Model: Transformer Block\n",
        "#    - Combine multi-head attention and MLP into a single Transformer block.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "450339bb-e193-4f73-892f-57516bfd2ee3",
        "id": "ycOcgoLhoakZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 16961\n",
            "Trainable Parameters: 16961\n",
            "Training loss: 4.153048992156982, Validation loss: 4.156349182128906\n",
            "Training loss: 2.447960615158081, Validation loss: 2.4581236839294434\n",
            "Training loss: 2.33398699760437, Validation loss: 2.3294832706451416\n",
            "Training loss: 2.2790815830230713, Validation loss: 2.3204283714294434\n",
            "Training loss: 2.2687125205993652, Validation loss: 2.312082290649414\n",
            "Training loss: 2.2480599880218506, Validation loss: 2.2877395153045654\n",
            "Training loss: 2.2398452758789062, Validation loss: 2.2740907669067383\n",
            "Training loss: 2.216015577316284, Validation loss: 2.2861952781677246\n",
            "Training loss: 2.192251205444336, Validation loss: 2.265817642211914\n",
            "Training loss: 2.1534619331359863, Validation loss: 2.255052328109741\n",
            "\n",
            "You to goodamby earsabluth say an heak no I torlans nothen ack, witin owouir to mee\n",
            "To for you an in there deart, vour tormesten as Row\n",
            "Must caill sher't shiciveme? dold gues,\n",
            "Histle,\n",
            "Ray nosur gaid t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Final Enhancements"
      ],
      "metadata": {
        "id": "p-y_u33roakZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. Model: Skip Connections, normalization and dropout"
      ],
      "metadata": {
        "id": "oId-MwMvoaka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Model: Skip Connections, normalization and dropout\n",
        "#    - Implement skip connections (residual connections) around the attention and MLP layers.\n",
        "#    - Add layer normalization before applying the skip connections to stabilize training.\n",
        "#    - Include dropout layers in both the attention and MLP layers to prevent overfitting.\n",
        "#    - Implement two or more Blocks.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cc5f83c-ceec-49cf-be74-45053411b01c",
        "id": "sonM_w5soaka"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 29761\n",
            "Trainable Parameters: 29761\n",
            "Training loss: 4.3595194816589355, Validation loss: 4.3730082511901855\n",
            "Training loss: 2.3575327396392822, Validation loss: 2.383897304534912\n",
            "Training loss: 2.2578649520874023, Validation loss: 2.2878003120422363\n",
            "Training loss: 2.2061879634857178, Validation loss: 2.1913561820983887\n",
            "Training loss: 2.166557550430298, Validation loss: 2.198211193084717\n",
            "Training loss: 2.1467902660369873, Validation loss: 2.212841272354126\n",
            "Training loss: 2.1245851516723633, Validation loss: 2.179342031478882\n",
            "Training loss: 2.102412223815918, Validation loss: 2.154122829437256\n",
            "Training loss: 2.104166269302368, Validation loss: 2.1471915245056152\n",
            "Training loss: 2.071061134338379, Validation loss: 2.143803119659424\n",
            "\n",
            "What thir, i Eswet pored\n",
            "Wend paserteld, thour in mearne 'Tur.\n",
            "What, curperfed,\n",
            "And I by good hron,\n",
            "So--fiar it stake maull how, Heram welf ouse be farrall ler hem,\n",
            "O'll, ime Vawadauche this him no sw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. Model: MultiHeadAttention [Alternate Implementation] (Optional)"
      ],
      "metadata": {
        "id": "csUuLW48VF_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. Model: MultiHeadAttention [Alternate Implementation] (Optional)\n",
        "#    - Implement MultiHeadAttention by utilizing a single class.\n",
        "#    - Refer to the following resource for guidance:\n",
        "#      https://github.com/rasbt/LLMs-from-scratch/tree/main/ch03/02_bonus_efficient-multihead-attention\n",
        "\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2qdwMCjLk_G",
        "outputId": "e0f61cef-d3f4-4d79-ecfc-5ae3aa6293f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 29761\n",
            "Trainable Parameters: 29761\n",
            "Training loss: 4.336919784545898, Validation loss: 4.352819919586182\n",
            "Training loss: 2.3357858657836914, Validation loss: 2.3310577869415283\n",
            "Training loss: 2.254905939102173, Validation loss: 2.2979040145874023\n",
            "Training loss: 2.1935226917266846, Validation loss: 2.2520251274108887\n",
            "Training loss: 2.171903610229492, Validation loss: 2.208240270614624\n",
            "Training loss: 2.1279537677764893, Validation loss: 2.1669797897338867\n",
            "Training loss: 2.1280019283294678, Validation loss: 2.155714511871338\n",
            "Training loss: 2.0721373558044434, Validation loss: 2.1721951961517334\n",
            "Training loss: 2.0832924842834473, Validation loss: 2.1373047828674316\n",
            "Training loss: 2.0340380668640137, Validation loss: 2.1342978477478027\n",
            "\n",
            "Fith andy mured'd Of ther,, wtuh, do deeadect? Wellove knablyw a sy seed.\n",
            "Your sumate dod peay your as Julaet, I\n",
            "To mece of all Fon con houldst; mhy his quend thour he wrowd hawell ight go maity your \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Final Training and Evaluation"
      ],
      "metadata": {
        "id": "03AgHiDcoaka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18. Final Evaluation and Text Generation"
      ],
      "metadata": {
        "id": "fyIFJtq4oaka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 18. Final Evaluation and Text Generation\n",
        "#    - Train the final GPT model with the complete architecture on a GPU for improved performance.\n",
        "#    - Model hyperparameters:\n",
        "#        - block_size: 256\n",
        "#        - n_embd: 128\n",
        "#        - n_head: 6\n",
        "#        - n_layer: 6\n",
        "#        - head_size: 16\n",
        "#    - Training hyperparameters:\n",
        "#        - batch_size: 128\n",
        "#        - max_iters: 5000\n",
        "#        - learning_rate: 1e-3\n",
        "#        - eval_interval: 500\n",
        "#        - eval_iters: 100\n",
        "#    - Evaluate the final model on the validation set at regular intervals.\n",
        "#    - Use the trained model to generate new text samples and assess its performance.\n",
        "\n",
        "# Follow the instructions and code the solution."
      ],
      "metadata": {
        "id": "LJLJfK8voaka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Solutions"
      ],
      "metadata": {
        "id": "3LRCJO8ucpFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setup and Data Preparation"
      ],
      "metadata": {
        "id": "eVehPJLRcpFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Imports & Configurations"
      ],
      "metadata": {
        "id": "xsdjjimqcpFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Imports & Configurations\n",
        "#    - Import necessary libraries (`torch`, `torch.nn`, `torch.nn.functional`, etc.).\n",
        "#    - Set device configuration (CPU/GPU).\n",
        "#    - Set random seed for reproducibility.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "print(f\"device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f0afc65-599c-43d4-be26-fe58c9715801",
        "id": "OdfM2GfLcpFk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Download Dataset"
      ],
      "metadata": {
        "id": "ySEBPepncpFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Download Dataset\n",
        "#    - Download and load the \"tinyshakespeare\" dataset.\n",
        "#    - https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "#    - Read the text data from the file.\n",
        "\n",
        "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27eb6344-dab4-46f1-caaf-910abb680694",
        "id": "L7zFD-xBcpFk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-23 13:48:01--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.3’\n",
            "\n",
            "input.txt.3         100%[===================>]   1.06M  5.89MB/s    in 0.2s    \n",
            "\n",
            "2024-08-23 13:48:02 (5.89 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Vocabulary Creation"
      ],
      "metadata": {
        "id": "1__rtWrCcpFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Vocabulary Creation\n",
        "#    - Extract unique characters from the dataset.\n",
        "#    - Determine the vocabulary size.\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)"
      ],
      "metadata": {
        "id": "w6kQw9IbcpFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Tokenizer"
      ],
      "metadata": {
        "id": "o62EHfu6cpFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Tokenizer\n",
        "#    - Create mappings for character-to-index (`stoi`) and index-to-character (`itos`).\n",
        "#    - Implement `encode()` and `decode()` functions for tokenization.\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "decode(encode('hello world!'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b3b475b2-bb2a-4deb-b822-9b1b63436493",
        "id": "75Jw_QVhcpFl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Train and Test Splits"
      ],
      "metadata": {
        "id": "zhIfDOCIcpFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Train and Test Splits\n",
        "#    - Convert the entire dataset into token indices.\n",
        "#    - Split the data into training and validation sets (90/10 split).\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(len(train_data), len(val_data))\n",
        "print(train_data[:10])\n",
        "print(val_data[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a45851e7-266e-4f5c-c7d0-e727f1576090",
        "id": "5t5n3ziOcpFl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1003854 111540\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n",
            "tensor([12,  0,  0, 19, 30, 17, 25, 21, 27, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Dataloader"
      ],
      "metadata": {
        "id": "fxIqnHIacpFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Dataloader\n",
        "#    - Define a function `get_batch()` to generate batches of data for training and evaluation.\n",
        "#    - Ensure that each batch contains sequences of fixed block size.\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 4\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix], dim=0)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix], dim=0)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x,y\n",
        "\n",
        "x, y = get_batch('train')\n",
        "x, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d688586-3a25-4fce-d411-e3dac521bf9d",
        "id": "ZMvxzHNpcpFl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
              "         [44, 53, 56,  1, 58, 46, 39, 58],\n",
              "         [52, 58,  1, 58, 46, 39, 58,  1],\n",
              "         [25, 17, 27, 10,  0, 21,  1, 54]], device='cuda:0'),\n",
              " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
              "         [53, 56,  1, 58, 46, 39, 58,  1],\n",
              "         [58,  1, 58, 46, 39, 58,  1, 46],\n",
              "         [17, 27, 10,  0, 21,  1, 54, 39]], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Building the Initial GPT Model"
      ],
      "metadata": {
        "id": "ABpJR4gPcpFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Model: Embedding Layer, Generate Function, Training Loop"
      ],
      "metadata": {
        "id": "gfLzUWMccpFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Consolidation\n",
        "# - Starting from this cell, we'll include all the code from 'Part 1: Setup and Data Preparation'\n",
        "#   at the beginning of each subsequent code cell. This approach ensures that we have a complete\n",
        "#   and up-to-date version of the entire codebase as we incrementally build the Transformer model.\n",
        "# - In addition to the data preparation code, we'll also include the model hyperparameters\n",
        "#   and training configurations in each cell.\n",
        "# - Going forward, always copy the complete code from the previous cell into the next one,\n",
        "#   and then add new features or enhancements. This method keeps the development process clear,\n",
        "#   and makes it easier to track progress and make modifications.\n",
        "# - This is the first cell where we begin consolidating code, so ensure that you bring over all\n",
        "#   necessary components from the previous steps as we continue to build on the Transformer.\n",
        "\n",
        "# 7. Model: Embedding Layer, Generate Function, Training Loop\n",
        "#    - Create a dataclass `GPTConfig` to store model hyperparameters.\n",
        "#    - Implement the GPT class with an embedding layer, producing logits for output.\n",
        "#    - Implement the `generate()` function to generate text using the trained model.\n",
        "#    - Instantiate a model.\n",
        "#    - Print the number of trainable parameters.\n",
        "#    - Pass one minibatch through the model.\n",
        "#    - Generate a sample from the model (output should be garbled/random initially).\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "#-------------------\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 4\n",
        "\n",
        "#-------------------\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"vocab size: {vocab_size}\")\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix], dim=0)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix], dim=0)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x,y\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTconfig:\n",
        "    n_embd:int = 32\n",
        "    vocab_size:int = vocab_size\n",
        "    block_size:int = block_size\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "\n",
        "        logits = self.token_embeddings(x)\n",
        "\n",
        "        if targets==None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=100):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -self.config.block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "        return idx\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "model = GPT(GPTconfig).to(device)\n",
        "\n",
        "total_parameters = sum(p.numel() for p in model.parameters())\n",
        "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
        "print(f\"Total Parameters: {total_parameters}\")\n",
        "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "logits, loss = model(xb, yb)\n",
        "\n",
        "starting_text = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(starting_text)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9b3616e-5287-4a02-d3c7-81f7e3f89ec0",
        "id": "mPMvXIPYcpFl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 4225\n",
            "Trainable Parameters: 4225\n",
            "\n",
            "pYCXxfRkRZd\n",
            "wc'wfNfT;OLlTEeC K\n",
            "jxqPToTb?bXAUG:C-SGJO-33SM:C?YI3a\n",
            "hs:LVXJFhXeNuwqhObxZ.tSVrddXlaSZaNe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Model: Training Loop"
      ],
      "metadata": {
        "id": "p5dPHBfncpFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Model: Training Loop\n",
        "#    - Set up the training loop with AdamW optimizer.\n",
        "#    - block_size:8, Batch size: 32, Training steps: 3,000, learning_rate: 1e-2\n",
        "#    - Generate sample from the model\n",
        "#    - Model output should have more structure after training\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "#-------------------\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 32\n",
        "\n",
        "training_steps = 3000\n",
        "learning_rate = 1e-2\n",
        "\n",
        "#-------------------\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"vocab size: {vocab_size}\")\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix], dim=0)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix], dim=0)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x,y\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTconfig:\n",
        "    n_embd:int = 32\n",
        "    vocab_size:int = vocab_size\n",
        "    block_size:int = block_size\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "\n",
        "        logits = self.token_embeddings(x)\n",
        "\n",
        "        if targets==None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=100):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -self.config.block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "        return idx\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "model = GPT(GPTconfig).to(device)\n",
        "\n",
        "total_parameters = sum(p.numel() for p in model.parameters())\n",
        "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
        "print(f\"Total Parameters: {total_parameters}\")\n",
        "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_steps):\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    _, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "print(f\"loss: {loss}\")\n",
        "\n",
        "starting_text = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(starting_text)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85629c00-d4d9-495e-b020-f85c3606f4ea",
        "id": "fPUfpJaNcpFl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 4225\n",
            "Trainable Parameters: 4225\n",
            "loss: 2.5201268196105957\n",
            "\n",
            "\n",
            "\n",
            "CEThik brid owindakis s, ble\n",
            "\n",
            "Hiset bube d e.\n",
            "S:\n",
            "O:3 my d?\n",
            "LUCous:\n",
            "Wanthar u qur, vet?\n",
            "F dXENDoate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Evaluation Loop"
      ],
      "metadata": {
        "id": "z1DwsjwCcpFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Evaluation Loop\n",
        "#    - Implement the `estimate_loss()` function to evaluate the model on training and validation data.\n",
        "#    - Ensure that the model is in evaluation mode during this process.\n",
        "#    - Set up the training loop as before.\n",
        "#    - Include periodic evaluation using `estimate_loss()` and print 10 training/validation losses.\n",
        "#    - Generate sample from the model.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "#-------------------\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 32\n",
        "\n",
        "training_steps = 3000\n",
        "learning_rate = 1e-2\n",
        "\n",
        "eval_interval = 300\n",
        "eval_steps = 50\n",
        "\n",
        "#-------------------\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"vocab size: {vocab_size}\")\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix], dim=0)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix], dim=0)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x,y\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTconfig:\n",
        "    n_embd:int = 32\n",
        "    vocab_size:int = vocab_size\n",
        "    block_size:int = block_size\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "\n",
        "        logits = self.token_embeddings(x)\n",
        "\n",
        "        if targets==None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=200):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -self.config.block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "        return idx\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "model = GPT(GPTconfig).to(device)\n",
        "\n",
        "total_parameters = sum(p.numel() for p in model.parameters())\n",
        "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
        "print(f\"Total Parameters: {total_parameters}\")\n",
        "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.ones(eval_steps)\n",
        "        for i in range(eval_steps):\n",
        "            x, y = get_batch(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[i] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_steps):\n",
        "\n",
        "    if (i % eval_interval == 0):\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Training loss: {losses['train']}, Validation loss: {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    _, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "starting_text = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(starting_text)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614e63df-f668-43a0-ebbf-8d82f8fe6c53",
        "id": "qoe-7cTEcpFn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 4225\n",
            "Trainable Parameters: 4225\n",
            "Training loss: 4.7332258224487305, Validation loss: 4.725982189178467\n",
            "Training loss: 2.7876455783843994, Validation loss: 2.8157029151916504\n",
            "Training loss: 2.5510098934173584, Validation loss: 2.5644922256469727\n",
            "Training loss: 2.491964340209961, Validation loss: 2.507617950439453\n",
            "Training loss: 2.4817771911621094, Validation loss: 2.515298843383789\n",
            "Training loss: 2.471799850463867, Validation loss: 2.4981491565704346\n",
            "Training loss: 2.487903594970703, Validation loss: 2.495173454284668\n",
            "Training loss: 2.47419810295105, Validation loss: 2.504297971725464\n",
            "Training loss: 2.4576833248138428, Validation loss: 2.494094133377075\n",
            "Training loss: 2.448154926300049, Validation loss: 2.496605157852173\n",
            "\n",
            "\n",
            "\n",
            "CExthy brid owindakis by ble\n",
            "\n",
            "Hisen bobe t e.\n",
            "S:\n",
            "O:3 my d?\n",
            "LUCous:\n",
            "Wanthar usqur, vet?\n",
            "F dXENDoate awice my.\n",
            "\n",
            "Hastacom oroup\n",
            "Yowhthetof is h ble mil ndill, ath iree s, hein lat Heridrovets, anend l \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Enhancing the GPT Model"
      ],
      "metadata": {
        "id": "ZwZEPn9tcpFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Model: Positional Embeddings"
      ],
      "metadata": {
        "id": "auAfuoBqcpFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Model: Positional Embeddings\n",
        "#    - Add positional embeddings to the model.\n",
        "#    - Train, evaluate and generate sample from the model.\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "#-------------------\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 32\n",
        "\n",
        "training_steps = 3000\n",
        "learning_rate = 1e-2\n",
        "\n",
        "eval_interval = 300\n",
        "eval_steps = 50\n",
        "\n",
        "#-------------------\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"vocab size: {vocab_size}\")\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix], dim=0)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix], dim=0)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x,y\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTconfig:\n",
        "    n_embd:int = 32\n",
        "    vocab_size:int = vocab_size\n",
        "    block_size:int = block_size\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.positional_embeddings = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.token_embeddings(x)\n",
        "        pos_emb = self.positional_embeddings(torch.arange(T, device=device))\n",
        "        emb = tok_emb + pos_emb\n",
        "        logits = self.lm_head(emb)\n",
        "\n",
        "        if targets==None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=200):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -self.config.block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "        return idx\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "model = GPT(GPTconfig).to(device)\n",
        "\n",
        "total_parameters = sum(p.numel() for p in model.parameters())\n",
        "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
        "print(f\"Total Parameters: {total_parameters}\")\n",
        "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.ones(eval_steps)\n",
        "        for i in range(eval_steps):\n",
        "            x, y = get_batch(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[i] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_steps):\n",
        "\n",
        "    if (i % eval_interval == 0):\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Training loss: {losses['train']}, Validation loss: {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    _, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "starting_text = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(starting_text)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d16eb139-5f4c-4f23-f7b5-37102c9b908e",
        "id": "WqHKLa6qcpFn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 4481\n",
            "Trainable Parameters: 4481\n",
            "Training loss: 4.474525451660156, Validation loss: 4.480035305023193\n",
            "Training loss: 2.522061347961426, Validation loss: 2.5282061100006104\n",
            "Training loss: 2.5352325439453125, Validation loss: 2.5343902111053467\n",
            "Training loss: 2.4970157146453857, Validation loss: 2.5091142654418945\n",
            "Training loss: 2.4937198162078857, Validation loss: 2.5417208671569824\n",
            "Training loss: 2.511420965194702, Validation loss: 2.5166428089141846\n",
            "Training loss: 2.5183358192443848, Validation loss: 2.521740198135376\n",
            "Training loss: 2.511129140853882, Validation loss: 2.526968002319336\n",
            "Training loss: 2.483370065689087, Validation loss: 2.5209455490112305\n",
            "Training loss: 2.472799062728882, Validation loss: 2.5132687091827393\n",
            "\n",
            "\n",
            "\n",
            "CExthy brid owindakis s, ble\n",
            "\n",
            "Hirenk obe d e.\n",
            "S:\n",
            "O:\n",
            "IS:\n",
            "Falatanss:\n",
            "Wanthar usqur he.\n",
            "War dilasoaten wice my.\n",
            "Whandarom oroug\n",
            "Yowns\n",
            "MERf inth ble mil ndilincath iree sengcin latisttid ovets, and Win \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Model: Single Attention Head"
      ],
      "metadata": {
        "id": "njNNeJzlcpFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Model: Single Attention Head\n",
        "#    - Implement a single attention head.\n",
        "#    - Train, evaluate and generate sample from the model.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "#-------------------\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 32\n",
        "\n",
        "training_steps = 3000\n",
        "learning_rate = 1e-2\n",
        "\n",
        "eval_interval = 300\n",
        "eval_steps = 50\n",
        "\n",
        "#-------------------\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"vocab size: {vocab_size}\")\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix], dim=0)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix], dim=0)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x,y\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTconfig:\n",
        "    n_embd:int = 32\n",
        "    vocab_size:int = vocab_size\n",
        "    block_size:int = block_size\n",
        "    head_size:int = 32\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.head_size = config.head_size\n",
        "        self.query = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "        self.key = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        attention_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
        "        attention_scores = torch.masked_fill(attention_scores, self.tril[:T, :T]==0, float('-inf'))\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        out = attention_weights @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.positional_embeddings = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.attention = Head(config)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.token_embeddings(x)\n",
        "        pos_emb = self.positional_embeddings(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.attention(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets==None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=200):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -self.config.block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "        return idx\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "model = GPT(GPTconfig).to(device)\n",
        "\n",
        "total_parameters = sum(p.numel() for p in model.parameters())\n",
        "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
        "print(f\"Total Parameters: {total_parameters}\")\n",
        "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.ones(eval_steps)\n",
        "        for i in range(eval_steps):\n",
        "            x, y = get_batch(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[i] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_steps):\n",
        "\n",
        "    if (i % eval_interval == 0):\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Training loss: {losses['train']}, Validation loss: {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    _, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "starting_text = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(starting_text)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a5668ac-2e77-4ddd-9f49-28c068e85118",
        "id": "cNivFgBmcpFn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 7553\n",
            "Trainable Parameters: 7553\n",
            "Training loss: 4.196639537811279, Validation loss: 4.2004075050354\n",
            "Training loss: 2.4888217449188232, Validation loss: 2.5333635807037354\n",
            "Training loss: 2.4639956951141357, Validation loss: 2.487450361251831\n",
            "Training loss: 2.433098077774048, Validation loss: 2.476203203201294\n",
            "Training loss: 2.4688873291015625, Validation loss: 2.4878597259521484\n",
            "Training loss: 2.417926549911499, Validation loss: 2.457453727722168\n",
            "Training loss: 2.422887086868286, Validation loss: 2.4499638080596924\n",
            "Training loss: 2.4246163368225098, Validation loss: 2.436007499694824\n",
            "Training loss: 2.3890421390533447, Validation loss: 2.4294393062591553\n",
            "Training loss: 2.425231456756592, Validation loss: 2.405696392059326\n",
            "\n",
            "Whent ikind\n",
            "Ocowr, hyo lay bth\n",
            "Y: ant bobe ale.\n",
            "S:\n",
            "O-'ts thalild\n",
            "hy ar hthar uwearthe.\n",
            "War dthay ate awicromy.\n",
            "\n",
            "HAEROYom onou waowns, tof itie botharl ndill, aes iree sen cie latiHet lrovets, and th p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Model: Multi-Head Attention"
      ],
      "metadata": {
        "id": "Z1l49uPkcpFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Model: Multi-Head Attention\n",
        "#    - Expand the model to include multiple attention heads.\n",
        "#    - Implement a projection layer to combine the outputs of the multiple heads.\n",
        "#    - Train, evaluate and generate sample from the model.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "#-------------------\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 32\n",
        "\n",
        "training_steps = 3000\n",
        "learning_rate = 1e-2\n",
        "\n",
        "eval_interval = 300\n",
        "eval_steps = 50\n",
        "\n",
        "#-------------------\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"vocab size: {vocab_size}\")\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix], dim=0)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix], dim=0)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x,y\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTconfig:\n",
        "    n_embd:int = 32\n",
        "    vocab_size:int = vocab_size\n",
        "    block_size:int = block_size\n",
        "    head_size:int = 8\n",
        "    n_heads:int = 4\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.head_size = config.head_size\n",
        "        self.query = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "        self.key = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        attention_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
        "        attention_scores = torch.masked_fill(attention_scores, self.tril[:T, :T]==0, float('-inf'))\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        out = attention_weights @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(config) for _ in range(config.n_heads)])\n",
        "        self.proj_o = nn.Linear(config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.proj_o(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.positional_embeddings = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.token_embeddings(x)\n",
        "        pos_emb = self.positional_embeddings(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.attention(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets==None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=200):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -self.config.block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "        return idx\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "model = GPT(GPTconfig).to(device)\n",
        "\n",
        "total_parameters = sum(p.numel() for p in model.parameters())\n",
        "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
        "print(f\"Total Parameters: {total_parameters}\")\n",
        "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.ones(eval_steps)\n",
        "        for i in range(eval_steps):\n",
        "            x, y = get_batch(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[i] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_steps):\n",
        "\n",
        "    if (i % eval_interval == 0):\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Training loss: {losses['train']}, Validation loss: {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    _, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "starting_text = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(starting_text)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396b369a-1098-42b6-81ff-59d0f2f78c11",
        "id": "XQo77EI9cpFo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 8609\n",
            "Trainable Parameters: 8609\n",
            "Training loss: 4.203001499176025, Validation loss: 4.203583240509033\n",
            "Training loss: 2.423476457595825, Validation loss: 2.4322385787963867\n",
            "Training loss: 2.3412063121795654, Validation loss: 2.374859571456909\n",
            "Training loss: 2.3324356079101562, Validation loss: 2.366330146789551\n",
            "Training loss: 2.2979342937469482, Validation loss: 2.339296817779541\n",
            "Training loss: 2.274425745010376, Validation loss: 2.30216646194458\n",
            "Training loss: 2.2752697467803955, Validation loss: 2.3007593154907227\n",
            "Training loss: 2.2543351650238037, Validation loss: 2.289290428161621\n",
            "Training loss: 2.2191426753997803, Validation loss: 2.277108907699585\n",
            "Training loss: 2.222029685974121, Validation loss: 2.274542808532715\n",
            "\n",
            "WAll be Rer\n",
            "wcowfach O la, bt madisen bobe to tarshr-' my calieanss:\n",
            "Whit If us hat vet?\n",
            "\n",
            "MEO:\n",
            "Do,\n",
            "Buswice my.\n",
            "\n",
            "Hand, I zo mus\n",
            "Yowns, to wit he me my wnd,\n",
            "Whates is ens, I in latiselidrev the he quing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Building the Transformer Block"
      ],
      "metadata": {
        "id": "LON9sv9qcpFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. Model: MLP"
      ],
      "metadata": {
        "id": "jC85iN9ocpFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Model: MLP\n",
        "#    - Add a multi-layer perceptron (MLP) to the model.\n",
        "#    - The MLP should consist of a projection up, ReLU activation, and a projection down.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "#-------------------\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 32\n",
        "\n",
        "training_steps = 3000\n",
        "learning_rate = 1e-2\n",
        "\n",
        "eval_interval = 300\n",
        "eval_steps = 50\n",
        "\n",
        "#-------------------\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"vocab size: {vocab_size}\")\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix], dim=0)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix], dim=0)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x,y\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTconfig:\n",
        "    n_embd:int = 32\n",
        "    vocab_size:int = vocab_size\n",
        "    block_size:int = block_size\n",
        "    head_size:int = 8\n",
        "    n_heads:int = 4\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.head_size = config.head_size\n",
        "        self.query = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "        self.key = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        attention_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
        "        attention_scores = torch.masked_fill(attention_scores, self.tril[:T, :T]==0, float('-inf'))\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        out = attention_weights @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(config) for _ in range(config.n_heads)])\n",
        "        self.proj_o = nn.Linear(config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.proj_o(out)\n",
        "        return out\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.up = nn.Linear(config.n_embd, config.n_embd * 4)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.down = nn.Linear(config.n_embd * 4, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.down(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.positional_embeddings = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "        self.mlp = MLP(config)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.token_embeddings(x)\n",
        "        pos_emb = self.positional_embeddings(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.attention(x)\n",
        "        x = self.mlp(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets==None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=200):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -self.config.block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "        return idx\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "model = GPT(GPTconfig).to(device)\n",
        "\n",
        "total_parameters = sum(p.numel() for p in model.parameters())\n",
        "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
        "print(f\"Total Parameters: {total_parameters}\")\n",
        "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.ones(eval_steps)\n",
        "        for i in range(eval_steps):\n",
        "            x, y = get_batch(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[i] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_steps):\n",
        "\n",
        "    if (i % eval_interval == 0):\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Training loss: {losses['train']}, Validation loss: {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    _, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "starting_text = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(starting_text)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726df867-6f0f-41c1-fbbc-efd1f1cff096",
        "id": "ZO9D8O17cpFo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 16961\n",
            "Trainable Parameters: 16961\n",
            "Training loss: 4.153048992156982, Validation loss: 4.156348705291748\n",
            "Training loss: 2.4448862075805664, Validation loss: 2.453115940093994\n",
            "Training loss: 2.3416080474853516, Validation loss: 2.3415679931640625\n",
            "Training loss: 2.2846858501434326, Validation loss: 2.320159912109375\n",
            "Training loss: 2.2464277744293213, Validation loss: 2.2876460552215576\n",
            "Training loss: 2.2493317127227783, Validation loss: 2.2965967655181885\n",
            "Training loss: 2.240849018096924, Validation loss: 2.2827556133270264\n",
            "Training loss: 2.2084546089172363, Validation loss: 2.274306535720825\n",
            "Training loss: 2.1917595863342285, Validation loss: 2.2594780921936035\n",
            "Training loss: 2.1379611492156982, Validation loss: 2.2440052032470703\n",
            "\n",
            "When befor you will to lay be mad and bobe do: and Or I lechaitangy: manth fou que hert?\n",
            "Wedtlah anes wice my thand a will mus\n",
            "You sproof is hy me mill dill, aeg is ens, hand lat Herid on to and I me \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. Model: Transformer Block"
      ],
      "metadata": {
        "id": "XxsGxpfVcpFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Model: Transformer Block\n",
        "#    - Combine multi-head attention and MLP into a single Transformer block.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "#-------------------\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 32\n",
        "\n",
        "training_steps = 3000\n",
        "learning_rate = 1e-2\n",
        "\n",
        "eval_interval = 300\n",
        "eval_steps = 50\n",
        "\n",
        "#-------------------\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"vocab size: {vocab_size}\")\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix], dim=0)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix], dim=0)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x,y\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTconfig:\n",
        "    n_embd:int = 32\n",
        "    vocab_size:int = vocab_size\n",
        "    block_size:int = block_size\n",
        "    head_size:int = 8\n",
        "    n_heads:int = 4\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.head_size = config.head_size\n",
        "        self.query = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "        self.key = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        attention_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
        "        attention_scores = torch.masked_fill(attention_scores, self.tril[:T, :T]==0, float('-inf'))\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        out = attention_weights @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(config) for _ in range(config.n_heads)])\n",
        "        self.proj_o = nn.Linear(config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.proj_o(out)\n",
        "        return out\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.up = nn.Linear(config.n_embd, config.n_embd * 4)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.down = nn.Linear(config.n_embd * 4, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.down(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attention(x)\n",
        "        x = self.mlp(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.positional_embeddings = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(config)\n",
        "        )\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.token_embeddings(x)\n",
        "        pos_emb = self.positional_embeddings(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets==None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=200):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -self.config.block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "        return idx\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "model = GPT(GPTconfig).to(device)\n",
        "\n",
        "total_parameters = sum(p.numel() for p in model.parameters())\n",
        "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
        "print(f\"Total Parameters: {total_parameters}\")\n",
        "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.ones(eval_steps)\n",
        "        for i in range(eval_steps):\n",
        "            x, y = get_batch(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[i] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_steps):\n",
        "\n",
        "    if (i % eval_interval == 0):\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Training loss: {losses['train']}, Validation loss: {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    _, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "starting_text = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(starting_text)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31a33a0f-1d9d-48a0-9cf7-6f276cef51cd",
        "id": "8wI18aHVcpFo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 16961\n",
            "Trainable Parameters: 16961\n",
            "Training loss: 4.153048992156982, Validation loss: 4.156348705291748\n",
            "Training loss: 2.4448862075805664, Validation loss: 2.453115940093994\n",
            "Training loss: 2.3416080474853516, Validation loss: 2.3415679931640625\n",
            "Training loss: 2.2846858501434326, Validation loss: 2.320159912109375\n",
            "Training loss: 2.2464277744293213, Validation loss: 2.2876460552215576\n",
            "Training loss: 2.2493317127227783, Validation loss: 2.2965967655181885\n",
            "Training loss: 2.240849018096924, Validation loss: 2.2827556133270264\n",
            "Training loss: 2.2084546089172363, Validation loss: 2.274306535720825\n",
            "Training loss: 2.1917595863342285, Validation loss: 2.2594780921936035\n",
            "Training loss: 2.1379611492156982, Validation loss: 2.2440052032470703\n",
            "\n",
            "When befor you will to lay be mad and bobe do: and Or I lechaitangy: manth fou que hert?\n",
            "Wedtlah anes wice my thand a will mus\n",
            "You sproof is hy me mill dill, aeg is ens, hand lat Herid on to and I me \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Final Enhancements"
      ],
      "metadata": {
        "id": "orpBvha4cpFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. Model: Skip Connections, normalization and dropout"
      ],
      "metadata": {
        "id": "CqbrUXrccpFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Model: Skip Connections, normalization and dropout\n",
        "#    - Implement skip connections (residual connections) around the attention and MLP layers.\n",
        "#    - Add layer normalization before applying the skip connections to stabilize training.\n",
        "#    - Include dropout layers in both the attention and MLP layers to prevent overfitting.\n",
        "#    - Implement two or more Blocks.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "#-------------------\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 32\n",
        "\n",
        "training_steps = 3000\n",
        "learning_rate = 1e-2\n",
        "\n",
        "eval_interval = 300\n",
        "eval_steps = 50\n",
        "\n",
        "#-------------------\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"vocab size: {vocab_size}\")\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix], dim=0)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix], dim=0)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x,y\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTconfig:\n",
        "    n_embd:int = 32\n",
        "    vocab_size:int = vocab_size\n",
        "    block_size:int = block_size\n",
        "    head_size:int = 8\n",
        "    n_heads:int = 4\n",
        "    n_layers:int = 2\n",
        "    dropout:float = 0.1\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.head_size = config.head_size\n",
        "        self.query = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "        self.key = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        attention_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
        "        attention_scores = torch.masked_fill(attention_scores, self.tril[:T, :T]==0, float('-inf'))\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "        out = attention_weights @ v\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(config) for _ in range(config.n_heads)])\n",
        "        self.proj_o = nn.Linear(config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        out = self.proj_o(out)\n",
        "        return out\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.up = nn.Linear(config.n_embd, config.n_embd * 4)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.down = nn.Linear(config.n_embd * 4, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.down(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "        self.mlp = MLP(config)\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dropout(self.attention(self.ln1(x)))\n",
        "        x = x + self.dropout(self.mlp(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.positional_embeddings = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.layers = nn.Sequential(*[Block(config) for _ in range(config.n_layers)])\n",
        "        self.ln = nn.LayerNorm(config.n_embd)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.token_embeddings(x)\n",
        "        pos_emb = self.positional_embeddings(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.layers(x)\n",
        "        x = self.ln(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets==None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=200):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -self.config.block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "        return idx\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "model = GPT(GPTconfig).to(device)\n",
        "\n",
        "total_parameters = sum(p.numel() for p in model.parameters())\n",
        "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
        "print(f\"Total Parameters: {total_parameters}\")\n",
        "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.ones(eval_steps)\n",
        "        for i in range(eval_steps):\n",
        "            x, y = get_batch(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[i] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_steps):\n",
        "\n",
        "    if (i % eval_interval == 0):\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Training loss: {losses['train']}, Validation loss: {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    _, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "starting_text = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(starting_text)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa4d6fc-84a1-4c57-c530-f44e637ba521",
        "id": "E8_LcabycpFp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 29761\n",
            "Trainable Parameters: 29761\n",
            "Training loss: 4.3595194816589355, Validation loss: 4.3730082511901855\n",
            "Training loss: 2.3513779640197754, Validation loss: 2.3574705123901367\n",
            "Training loss: 2.256624698638916, Validation loss: 2.2850542068481445\n",
            "Training loss: 2.2053983211517334, Validation loss: 2.242753267288208\n",
            "Training loss: 2.1784188747406006, Validation loss: 2.2001447677612305\n",
            "Training loss: 2.1738219261169434, Validation loss: 2.2290825843811035\n",
            "Training loss: 2.134580135345459, Validation loss: 2.204617738723755\n",
            "Training loss: 2.105912446975708, Validation loss: 2.1689751148223877\n",
            "Training loss: 2.07553768157959, Validation loss: 2.161000967025757\n",
            "Training loss: 2.078049421310425, Validation loss: 2.1510331630706787\n",
            "\n",
            "LUCIONLRDICHARD IOLONTEL:\n",
            "My, ther he lome.\n",
            "Fare brout rhibt:\n",
            "Fhorssty noks my how will of My batood oust goody\n",
            "Costown.\n",
            "\n",
            "DUKINCE:\n",
            "But miclay ten.\n",
            "\n",
            "ORKKE LARLUCIO:\n",
            "Sward,\n",
            "Mut they liveing;\n",
            "He lery you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. Model: MultiHeadAttention [Alternate Implementation] (Optional)"
      ],
      "metadata": {
        "id": "sUwWzX49cpFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. Model: MultiHeadAttention [Alternate Implementation] (Optional)\n",
        "#    - Implement MultiHeadAttention by utilizing a single class.\n",
        "#    - Refer to the following resource for guidance:\n",
        "#      https://github.com/rasbt/LLMs-from-scratch/tree/main/ch03/02_bonus_efficient-multihead-attention\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "#-------------------\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 32\n",
        "\n",
        "training_steps = 3000\n",
        "learning_rate = 1e-2\n",
        "\n",
        "eval_interval = 300\n",
        "eval_steps = 50\n",
        "\n",
        "#-------------------\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"vocab size: {vocab_size}\")\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix], dim=0)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix], dim=0)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x,y\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTconfig:\n",
        "    n_embd:int = 32\n",
        "    vocab_size:int = vocab_size\n",
        "    block_size:int = block_size\n",
        "    head_size:int = 8\n",
        "    n_heads:int = 4\n",
        "    n_layers:int = 2\n",
        "    dropout:float = 0.1\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.head_size = config.head_size\n",
        "        self.n_heads = config.n_heads\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "        self.o_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x).view(B, T, self.n_heads, self.head_size)\n",
        "        k = self.key(x).view(B, T, self.n_heads, self.head_size)\n",
        "        v = self.value(x).view(B, T, self.n_heads, self.head_size)\n",
        "\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "        # (B, n_heads, T, head_size)\n",
        "\n",
        "        attention_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
        "        # (B, n_heads, T, T)\n",
        "        attention_scores = attention_scores.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        out = attention_weights @ v\n",
        "        # (B, n_heads, T, head_size)\n",
        "        out = out.transpose(1,2)\n",
        "        # (B, T, n_heads, head_size)\n",
        "        out = out.contiguous().view(B, T, C)\n",
        "\n",
        "        out = self.o_proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.up = nn.Linear(config.n_embd, config.n_embd * 4)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.down = nn.Linear(config.n_embd * 4, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.down(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "        self.mlp = MLP(config)\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dropout(self.attention(self.ln1(x)))\n",
        "        x = x + self.dropout(self.mlp(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.positional_embeddings = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.layers = nn.Sequential(*[Block(config) for _ in range(config.n_layers)])\n",
        "        self.ln = nn.LayerNorm(config.n_embd)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.token_embeddings(x)\n",
        "        pos_emb = self.positional_embeddings(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.layers(x)\n",
        "        x = self.ln(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets==None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=200):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -self.config.block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "        return idx\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "model = GPT(GPTconfig).to(device)\n",
        "\n",
        "total_parameters = sum(p.numel() for p in model.parameters())\n",
        "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
        "print(f\"Total Parameters: {total_parameters}\")\n",
        "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.ones(eval_steps)\n",
        "        for i in range(eval_steps):\n",
        "            x, y = get_batch(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[i] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_steps):\n",
        "\n",
        "    if (i % eval_interval == 0):\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Training loss: {losses['train']}, Validation loss: {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    _, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "starting_text = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(starting_text)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b6e17b-1434-431c-8c75-d9d106b14908",
        "id": "pPZKH8wxcpFp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 29761\n",
            "Trainable Parameters: 29761\n",
            "Training loss: 4.336919784545898, Validation loss: 4.352819919586182\n",
            "Training loss: 2.361799955368042, Validation loss: 2.358400583267212\n",
            "Training loss: 2.2427780628204346, Validation loss: 2.2684075832366943\n",
            "Training loss: 2.1912455558776855, Validation loss: 2.236848831176758\n",
            "Training loss: 2.1693694591522217, Validation loss: 2.197221279144287\n",
            "Training loss: 2.1507184505462646, Validation loss: 2.2159225940704346\n",
            "Training loss: 2.1148903369903564, Validation loss: 2.1888115406036377\n",
            "Training loss: 2.085590362548828, Validation loss: 2.1606204509735107\n",
            "Training loss: 2.0637192726135254, Validation loss: 2.156837224960327\n",
            "Training loss: 2.0486533641815186, Validation loss: 2.1241254806518555\n",
            "\n",
            "He blive, the thou! file have poun Froy tarrast chank and the stoond sorome I but thensees\n",
            "h arefe horce,\n",
            "Dhee, thangountly my nothis troe gleBe the duy: as to, bale rechath are,\n",
            "Lintar betwle if-f, y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Final Training and Evaluation"
      ],
      "metadata": {
        "id": "Iy9tBVLjcpFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18. Final Evaluation and Text Generation"
      ],
      "metadata": {
        "id": "DKfXf4eGcpFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 18. Final Evaluation and Text Generation\n",
        "#    - Train the final GPT model with the complete architecture on a GPU for improved performance.\n",
        "#    - Model hyperparameters:\n",
        "#        - block_size: 256\n",
        "#        - n_embd: 128\n",
        "#        - n_head: 6\n",
        "#        - n_layer: 6\n",
        "#        - head_size: 16\n",
        "#    - Training hyperparameters:\n",
        "#        - batch_size: 128\n",
        "#        - max_iters: 5000\n",
        "#        - learning_rate: 1e-3\n",
        "#        - eval_interval: 500\n",
        "#        - eval_iters: 100\n",
        "#    - Evaluate the final model on the validation set at regular intervals.\n",
        "#    - Use the trained model to generate new text samples and assess its performance.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "#-------------------\n",
        "\n",
        "block_size = 256\n",
        "batch_size = 128\n",
        "\n",
        "training_steps = 5000\n",
        "learning_rate = 1e-3\n",
        "\n",
        "eval_interval = 500\n",
        "eval_steps = 50\n",
        "\n",
        "#-------------------\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"vocab size: {vocab_size}\")\n",
        "\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "n = int(0.9*len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix], dim=0)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix], dim=0)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x,y\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTconfig:\n",
        "    n_embd:int = 128\n",
        "    vocab_size:int = vocab_size\n",
        "    block_size:int = block_size\n",
        "    head_size:int = 16\n",
        "    n_heads:int = 8\n",
        "    n_layers:int = 4\n",
        "    dropout:float = 0.1\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.head_size = config.head_size\n",
        "        self.n_heads = config.n_heads\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "        self.o_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.query(x).view(B, T, self.n_heads, self.head_size)\n",
        "        k = self.key(x).view(B, T, self.n_heads, self.head_size)\n",
        "        v = self.value(x).view(B, T, self.n_heads, self.head_size)\n",
        "\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "        # (B, n_heads, T, head_size)\n",
        "\n",
        "        attention_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
        "        # (B, n_heads, T, T)\n",
        "        attention_scores = attention_scores.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        out = attention_weights @ v\n",
        "        # (B, n_heads, T, head_size)\n",
        "        out = out.transpose(1,2)\n",
        "        # (B, T, n_heads, head_size)\n",
        "        out = out.contiguous().view(B, T, C)\n",
        "\n",
        "        out = self.o_proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.up = nn.Linear(config.n_embd, config.n_embd * 4)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.down = nn.Linear(config.n_embd * 4, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.down(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "        self.mlp = MLP(config)\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.dropout(self.attention(self.ln1(x)))\n",
        "        x = x + self.dropout(self.mlp(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config:GPTconfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.positional_embeddings = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.layers = nn.Sequential(*[Block(config) for _ in range(config.n_layers)])\n",
        "        self.ln = nn.LayerNorm(config.n_embd)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "\n",
        "        B, T = x.shape\n",
        "\n",
        "        tok_emb = self.token_embeddings(x)\n",
        "        pos_emb = self.positional_embeddings(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.layers(x)\n",
        "        x = self.ln(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets==None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=200):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_clipped = idx[:, -self.config.block_size:]\n",
        "            logits, loss = self(idx_clipped)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=-1)\n",
        "        return idx\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "model = GPT(GPTconfig).to(device)\n",
        "\n",
        "total_parameters = sum(p.numel() for p in model.parameters())\n",
        "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad==True)\n",
        "print(f\"Total Parameters: {total_parameters}\")\n",
        "print(f\"Trainable Parameters: {trainable_parameters}\")\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.ones(eval_steps)\n",
        "        for i in range(eval_steps):\n",
        "            x, y = get_batch(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[i] = loss\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(training_steps):\n",
        "\n",
        "    if (i % eval_interval == 0):\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Training loss: {losses['train']}, Validation loss: {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    _, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "starting_text = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(starting_text)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrhybl7ycpFp",
        "outputId": "c4556676-bb69-402b-ba7d-da0098a1a94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 65\n",
            "Total Parameters: 841281\n",
            "Trainable Parameters: 841281\n",
            "Training loss: 4.32258415222168, Validation loss: 4.3253092765808105\n",
            "Training loss: 1.9829299449920654, Validation loss: 2.0749642848968506\n",
            "Training loss: 1.6285208463668823, Validation loss: 1.7984155416488647\n",
            "Training loss: 1.4782105684280396, Validation loss: 1.6699893474578857\n",
            "Training loss: 1.4023021459579468, Validation loss: 1.6121479272842407\n",
            "Training loss: 1.3509314060211182, Validation loss: 1.584789752960205\n",
            "Training loss: 1.3177154064178467, Validation loss: 1.553526759147644\n",
            "Training loss: 1.283528447151184, Validation loss: 1.5408179759979248\n",
            "Training loss: 1.2645879983901978, Validation loss: 1.5332915782928467\n",
            "Training loss: 1.243046522140503, Validation loss: 1.5287652015686035\n",
            "\n",
            "Then remies from thy hope shall be clock,\n",
            "That we enjoy, nurse Marcius thy father's,\n",
            "For Alike, our protection how of lend\n",
            "As as fingly? why, I pray even up his\n",
            "New from that flour him and miserous\n",
            "Th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "starting_text = torch.tensor([[0]], dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(model.generate(starting_text, max_new_tokens=5000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcaxhu9RmNO2",
        "outputId": "85c6f9af-6f34-4816-9d5a-e38a478555b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "First Most Cominius' to our plate.\n",
            "\n",
            "JULIET:\n",
            "O you let the riving of you.\n",
            "\n",
            "ROMEO:\n",
            "I will't even it word by the baitip of soul,\n",
            "Which it would be solemness, and think upon,\n",
            "A poloybrouse Clarence! Dedarest none:\n",
            "You woo must on the little of you?\n",
            "\n",
            "Both:\n",
            "Compare adument And that most perfeige.\n",
            "\n",
            "Second Citizen:\n",
            "What news? cousin, I had, what a produce smate,\n",
            "Harking, ladier, what's the fault, who lord mays?\n",
            "\n",
            "MENENIUS:\n",
            "What cannot! told what's young axard together?\n",
            "\n",
            "CORIOLANUS::\n",
            "Lest your joyful forty didst-beshored\n",
            "To jewel than readine your town: she was here,\n",
            "Would come of makes, if he hoppy and firns\n",
            "And kness'd two dies, and what thy words;\n",
            "What would then father thanks of have stoud hath\n",
            "Found of bids, or brother no pitience.\n",
            "Or I,neverien not to chose;\n",
            "But thy contracte?\n",
            "\n",
            "Lover:\n",
            "O, if he is news,\n",
            "As yea; for these way: nurse may deserves the heaven my\n",
            "With such corse-havir beats, nay bearthing down,\n",
            "Figmend them as lid change affawel.\n",
            "\n",
            "Clifford:\n",
            "How none laws to his great air brook all\n",
            "God him majesty.\n",
            "\n",
            "CAPULET:\n",
            "Now, child!--\n",
            "\n",
            "Nurse:\n",
            "Well.\n",
            "\n",
            "JULIET:\n",
            "Nay here?\n",
            "\n",
            "KING GARD VII:\n",
            "Ay, what sick or Richard: you have that?\n",
            "Adde make you, good Pompey, you joy\n",
            "All turn to a larrity. Come, brow action;\n",
            "And upon that swears will so ry darge;\n",
            "Or give bathor, hence, I will and friar,\n",
            "To turn sweak thus widow rag out.\n",
            "\n",
            "CLARENCE:\n",
            "I canot thy bear which you utranch 'You\n",
            "Or broth: when to bark,'d none.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Addieu; it is cournsel this love?\n",
            "\n",
            "BUSHY:\n",
            "Where is my salt comes to adversoe;\n",
            "Where advenCAPULANT:\n",
            "Belike my Mantua, uncle, in Corage,\n",
            "That is too fexers, here's his fancy; and he's pity fond\n",
            "A office-frame-landed like so abroacle:\n",
            "Away; at they ember to my foight at man.\n",
            "\n",
            "CLIFFOND:\n",
            "He rounds not\n",
            "Not part. Marcius! No, I do beseast.\n",
            "\n",
            "LUCIO:\n",
            "Beauch'd he had rather, and I set myself.\n",
            "\n",
            "CAPULET:\n",
            "How! what is God my leaves but bite to\n",
            "Now the rour cape: I'll love us him;\n",
            "For 'twill my boot; while have sport me know\n",
            "Because to his compion, e'er fo last,\n",
            "Is mine and a dares,\n",
            "And their kept of thy charses against of him;\n",
            "Eive ever mine honour bitted with which;\n",
            "That, hast you may once I do will follow\n",
            "Thy childremation\n",
            "To crow on Angulward's queen.\n",
            "\n",
            "TYBALT:\n",
            "Turn morrow make a folks; adieuty\n",
            "To thank him that that the osence could have\n",
            "Prefuse threat talk thy boundard of cousin;\n",
            "For they will mou ray she was,\n",
            "Not led cry a wing, great night, courself true,\n",
            "And let them shall watchood Broof, to faitht forth;\n",
            "Nay, my father fly too look to helply while.\n",
            "\n",
            "RIVERS:\n",
            "I nernews upon the world.\n",
            "\n",
            "KING EDWARD IV:\n",
            "But is not voice; your poor new have more,\n",
            "To thoughts in din yet our grace fame,\n",
            "Lark Station, that is that it tears soul,\n",
            "Gentleman the Lord Henry lost storm in him plot:\n",
            "There's fant, which they have hath to me en.\n",
            "\n",
            "KING HENRY VI:\n",
            "Ay, your commonded naith! What afoder them?\n",
            "My upon us are curtures of his comfort than\n",
            "That device was in his park frail; if he comes to\n",
            "Your princely heart, on that at hat bravely\n",
            "To shall scattle of boot: a tother is\n",
            "graves, and eytay and thee heart fall from their full\n",
            "I glay my hate\n",
            "And are where heaven of Rome, my lord.\n",
            "If'll bring them have took from your blows?\n",
            "\n",
            "DERCAPUCENCE:\n",
            "And I cry your notherity! Why wrong'dly woe!\n",
            "\n",
            "PRINCE EDWARD:\n",
            "My lord, by God thus, my lord, to I\n",
            "Of Soundamer, come, and have Dore.\n",
            "\n",
            "Nurse:\n",
            "Have you no moont.\n",
            "\n",
            "CAPULET:\n",
            "From do doth a hand:\n",
            "See a primonely ay, it betted them falk\n",
            "Fith given the air waid bed to\n",
            "The grand; mine is suffect that they and is?\n",
            "\n",
            "Nurse:\n",
            "Now, nor ever.\n",
            "\n",
            "ROMEO:\n",
            "O lore, my rage is good word--\n",
            "Poor of Tranio, some warrant his should were the\n",
            "Since for Rome in the sweet are so out\n",
            "Which look foil corrents have ted downful\n",
            "Of wixon'd see the morn!\n",
            "\n",
            "JULIET:\n",
            "A rither ix too; for those and his court\n",
            "Most for willingly good flower follow,\n",
            "Look than fair, that thoughts Capules?\n",
            "\n",
            "ROMEO:\n",
            "Affection, my father,\n",
            "Or all that tull your book truth,\n",
            "That his noble houses would death?\n",
            "The gods affriar, yet is Port? and glast\n",
            "Against melt I thunded a bab.\n",
            "Will tell, from what that I go to make\n",
            "By my majesty of his at theman us,\n",
            "Fall or Lord Warwick thee nothing on daubted.\n",
            "\n",
            "And well:\n",
            "I, not phy secre your brick a little a frank,\n",
            "And many doth a noble world kiss of hist;\n",
            "For never cover vow: this born but witing my\n",
            "That youth do to be with the Vols' tears?\n",
            "Or: I have it, it metether thy in thank\n",
            "That way't and for our mind\n",
            "As fortune should joins, a viritenant wife\n",
            "Thou jet far.\n",
            "\n",
            "CLARENCE:\n",
            "Good graces it beseech that o'erparine.\n",
            "\n",
            "GLOUCESTER:\n",
            "Spear; Farewell! think when then?\n",
            "\n",
            "PAULINA:\n",
            "My prisoner of some blame feels\n",
            "You have not unfury to bridge.\n",
            "What, indire againstan my nack?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Then be the word cause: what you have said\n",
            "Left nother maid for here, when I rath, Warwick, our\n",
            "Would of eace never, towards are, solak, and there?\n",
            "Covery come to your must seeing strow out,\n",
            "Your Polixeo:\n",
            "They are a near, a rishoyal warrant of furnishments?\n",
            "\n",
            "First Murderer:\n",
            "Sweet, what strays would flatter know forge\n",
            "To bitter droom. And thou to are, Bianca,\n",
            "back'd \n"
          ]
        }
      ]
    }
  ]
}